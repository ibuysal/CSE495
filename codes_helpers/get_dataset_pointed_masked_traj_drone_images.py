# -*- coding: utf-8 -*-
"""Copy of iamge_feature.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_AyYoIOZP8zGs81vXSHJsBvS-tnvs94i

taejectory featuere as a imge from observer drone. trained and tested on 40 20 noise on same distance.
"""

# #mount drive
# from google.colab import drive
# drive.mount('/content/drive')

import os
import cv2
import numpy as np
import pandas as pd


import os

# 1. Ana klasörde "images" ile biten klasörleri bul
def find_images_dirs(base_dir):
    images_dirs = []
    for root, dirs, files in os.walk(base_dir):
        for dir_name in dirs:
            if dir_name == "images":  # Match exactly "images"
                images_dirs.append(os.path.join(root, dir_name))
    return images_dirs

# 2. Her "images" klasörüne karşılık gelen CSV dosyasını ara (isim içinde eşleşme)
def find_matching_csv(images_dirs, csv_dir):
    matches = {}

    for image_dir in images_dirs:
        # "images" kelimesini kaldır ve klasör ismini al
        path_parts = image_dir.split('/')
        print(path_parts)
    # Find the index where 'images' appears
        images_index = path_parts.index('images')

        # Get the two directories before 'images'
        experiment_path = '/'.join(path_parts[images_index-2:images_index-1])
        folder_prefix = experiment_path
        #print(folder_prefix)
        # CSV dosyalarını tara
        matched_csv = None
        for csv_file in sorted(os.listdir(csv_dir)):
            if csv_file.endswith(".csv") and folder_prefix in csv_file:
                matched_csv = os.path.join(csv_dir, csv_file)
                break  # İlk eşleşmeyi bulduktan sonra çık

        matches[image_dir] = matched_csv  # Eşleşme yoksa None olarak eklenir

    return matches

# 3. Kullanım
import numpy as np
import pandas as pd
def extract_advanced_trajectory_features(trajectory):
   # Distance metrics
   total_length = np.sum(np.sqrt(np.sum(np.diff(trajectory, axis=0)**2, axis=1)))
   direct_dist = np.linalg.norm(trajectory[-1] - trajectory[0])
   sinuosity = total_length / direct_dist

   # Turning analysis
   vectors = np.diff(trajectory, axis=0)
   angles = np.arctan2(vectors[1:,1], vectors[1:,0]) - np.arctan2(vectors[:-1,1], vectors[:-1,0])
   mean_angle = np.mean(angles)
   angle_var = np.var(angles)

   # Spatial distribution
   centroid = np.mean(trajectory, axis=0)
   r_gyr = np.sqrt(np.mean(np.sum((trajectory - centroid)**2, axis=1)))
   bbox = np.max(trajectory, axis=0) - np.min(trajectory, axis=0)

   # Key points
   start = trajectory[0]
   end = trajectory[-1]
   mid = trajectory[len(trajectory)//2]
   height_var = np.max(trajectory[:,2]) - np.min(trajectory[:,2])

   features = np.concatenate([
       [total_length, direct_dist, sinuosity],  # 3
       [mean_angle, angle_var],                 # 2
       centroid,                                # 3
       [r_gyr],                                 # 1
       bbox,                                    # 3
       start, mid, end,                         # 9
       [height_var]                             # 1
   ])

   return features

def load_trajectories(matches):
    """
    Load all trajectories once and store them with unique IDs
    """
    trajectories = {}

    for image_dir, csv_path in matches.items():
        if csv_path:
            data_dir = os.path.dirname(image_dir)
            xyz_path = os.path.join(data_dir, 'iris1_XYZ.csv')
            # #print(f"xyz_path {xyz_path}")
            # Generate unique ID for this trajectory (using directory name)
            trajectory_dir_name = os.path.dirname(xyz_path)
            trajectory_id = (os.path.dirname(trajectory_dir_name))
            trajectory_id=os.path.basename(trajectory_id)
            # #print(trajectory_id)
            # Load trajectory if not already loaded
            if trajectory_id not in trajectories:
                # Read XYZ data
                trajectory_df = pd.read_csv(xyz_path)
                trajectory_df = trajectory_df.sort_values(by=['Sec', 'Nanosec']).reset_index(drop=True)
                trajectory_points = trajectory_df[['Position X', 'Position Y', 'Position Z']].values

                # Store trajectory
                traj_image_path=os.path.join(data_dir,"iris1_trajectory.jpg")
                # #print(traj_image_path)
                trajectories[trajectory_id] = {
                    'points': trajectory_points,
                    'start_time': trajectory_df.iloc[0]['Sec'],
                    'end_time': trajectory_df.iloc[-1]['Sec'],
                    'image_path': traj_image_path,
                    'features': extract_advanced_trajectory_features(trajectory_points)
                }

                #print(f"Loaded trajectory {trajectory_id}: {trajectory_points.shape} points")

    return trajectories


def find_trajectory_point_and_next(timestamp_sec, timestamp_nanosec, trajectory_df):
    """
    Find current and next point in trajectory based on timestamp

    Args:
        timestamp_sec: Current second timestamp
        timestamp_nanosec: Current nanosecond timestamp
        trajectory_df: DataFrame containing trajectory points with Sec, Nanosec columns

    Returns:
        tuple: (current_point, next_point) where each point is a numpy array [x,y,z]
    """
    # Calculate time difference for each point in seconds
    time_diff = (trajectory_df['Sec'] - timestamp_sec) + \
                (trajectory_df['Nanosec'] - timestamp_nanosec) * 1e-9

    # Find index of closest point (smallest absolute time difference)
    current_idx = abs(time_diff).argmin()

    # Get next index (if current is last point, use same point)
    next_idx = min(current_idx + 1, len(trajectory_df) - 1)

    # Get current and next points
    current_point = trajectory_df.iloc[current_idx][['Position X', 'Position Y', 'Position Z']].values
    next_point = trajectory_df.iloc[next_idx][['Position X', 'Position Y', 'Position Z']].values

    return current_point, next_point
def create_trajectory_dataset(matches):
    """
    Create dataset using both specific target points and full trajectory
    """
    image_paths = []
    iris_coords = []
    iris1_coords = []
    target_points = []
    next_points = []
    trajectory_ids = []
    source_files = []

    for image_dir, csv_path in matches.items():
        if csv_path:
            #print(f"Processing {csv_path}")
            # Read main CSV
            df = pd.read_csv(csv_path)
            df = df.sort_values(by=['image_sec', 'image_nanosec']).reset_index(drop=True)

            # Read trajectory data
            data_dir = os.path.dirname(image_dir)
            xyz_path = os.path.join(data_dir, 'iris1_XYZ.csv')
            #print(f"xyz_path {xyz_path}")

            trajectory_dir_name = os.path.dirname(xyz_path)
            trajectory_id = (os.path.dirname(trajectory_dir_name))
            trajectory_id=os.path.basename(trajectory_id)

            trajectory_df = pd.read_csv(xyz_path)
            trajectory_df = trajectory_df.sort_values(by=['Sec', 'Nanosec']).reset_index(drop=True)

            # Get full trajectory points
            full_trajectory = trajectory_df[['Position X', 'Position Y', 'Position Z']].values

            start_time = trajectory_df.iloc[0]['Sec'] + trajectory_df.iloc[0]['Nanosec'] * 1e-9
            end_time = trajectory_df.iloc[-1]['Sec'] + trajectory_df.iloc[-1]['Nanosec'] * 1e-9 +1.2

            first_image=None
            drone_pixels=[]
            
                

            for _, row in df.iterrows():
                current_time = row['image_sec'] + row['image_nanosec'] * 1e-9
                if start_time <= current_time <= end_time:  
                    img_path = os.path.join(image_dir, row['image'])
                    if os.path.exists(img_path):
                        # Get current and next trajectory points
                        current_point, next_point = find_trajectory_point_and_next(
                            row['image_sec'],
                            row['image_nanosec'],
                            trajectory_df
                        )

                        # Get observer and target coordinates
                        x1, y1, z1 = row['x_iris'], row['y_iris'], -row['z_iris']
                        x2, y2, z2 = row['x_iris1'],row['y_iris'], row['z_iris1']
                        # #print(img_path,csv_path,trajectory_id)

                        image_paths.append(img_path)
                        iris_coords.append((round(x1,3), round(y1,3), round(z1,3)))
                        iris1_coords.append((round(x2,3), round(y2,3), round(z2,3)))
                        target_points.append(current_point)
                        next_points.append(next_point)
                        trajectory_ids.append(trajectory_id)
                        source_files.append(csv_path)

    return image_paths, iris_coords, iris1_coords, target_points, next_points, trajectory_ids, source_files


def split_train_val_dataset(data_tuple, trajectories, val_ratio=0.4):
    """
    Split dataset into train and validation sets using trajectory IDs

    Args:
        data_tuple: Tuple of (image_paths, iris_coords, iris1_coords, target_points, next_points, trajectories, source_files)
        trajectories: Dictionary of trajectory information from load_trajectories
        val_ratio: Ratio of data to use for validation
    Returns:
        Tuple of (train_data, val_data)
    """
    # Get trajectory IDs from our loaded trajectories
    trajectory_ids = list(trajectories.keys())
    #print(f"trajectory_ids {trajectory_ids}")
    # Shuffle the trajectory IDs
    np.random.seed(42)
    np.random.shuffle(trajectory_ids)

    # Split trajectory IDs into train and validation
    val_size = int(len(trajectory_ids) * val_ratio)
    train_traj_ids = set(trajectory_ids[val_size:])
    val_traj_ids = set(trajectory_ids[:val_size])

    # #print(f"\nDataset split:")
    # #print(f"Training trajectories: {len(train_traj_ids)}")
    # #print(f"Validation trajectories: {len(val_traj_ids)}")
    # #print(f"Training trajectories: {train_traj_ids}")
    # #print(f"Validation trajectories: {val_traj_ids}")

    # # Get source files
    source_files = data_tuple[0]

    # Create masks for train and validation sets based on trajectory IDs
    train_mask = []
    val_mask = []

    for i, source_file in enumerate(source_files):
        # Extract trajectory ID from source file path

        trajectory_dir_name = os.path.dirname(source_file)
        base_dir = os.path.dirname(trajectory_dir_name)
        base_dir = os.path.dirname(base_dir)

        traj_id = os.path.basename(base_dir)
        #print(trajectory_dir_name,base_dir,traj_id)
        if traj_id in train_traj_ids:
            train_mask.append(i)
        elif traj_id in val_traj_ids:
            val_mask.append(i)

    # Function to apply mask to data
    def apply_mask(data, mask):
        if isinstance(data, (list, tuple)):
            return [data[i] for i in mask]
        return [data[i] for i in mask]

    # Split all data using masks

    train_data = tuple(apply_mask(d, train_mask) for d in data_tuple)
    val_data = tuple(apply_mask(d, val_mask) for d in data_tuple)
    # #print(train_data)
    # #print(f"Training samples: {len(train_data[0])}")
    # #print(f"Validation samples: {len(val_data[0])}")

    return train_data, val_data




base_dir = "/home/ibu/bitirme_ws/traj_data_final_ekleme"  # "images" klasörlerinin olduğu ana klasör
csv_dir = "/home/ibu/bitirme_ws/traj_data_final_ekleme_csv"   # CSV dosyalarının olduğu klasör

# Adım 1: "images" klasörlerini bul
images_dirs = find_images_dirs(base_dir)
#print(f"Bulunan 'images' klasörleri: {images_dirs}")
#find base directory
import os
data_dir= [os.path.dirname(path) for path in images_dirs]
# Adım 2: Eşleşen CSV dosyalarını bul
matches = find_matching_csv(images_dirs, csv_dir)
#print(matches)

# print(matches)
# 3. Kullanım
# base_dir_test = "/home/ibu/bitirme_ws/test_20_30"  # "images" klasörlerinin olduğu ana klasör
# csv_dir_test = "/home/ibu/bitirme_ws/test_20_30_csv"   # CSV dosyalarının olduğu klasör

# # Adım 1: "images" klasörlerini bul
# images_dirs_test = find_images_dirs(base_dir_test)
# #print(f"Bulunan 'images' klasörleri: {images_dirs_test}")
# #find base directory
# import os
# data_dir= [os.path.dirname(path) for path in images_dirs_test]
# # Adım 2: Eşleşen CSV dosyalarını bul
# matches_test = find_matching_csv(images_dirs_test, csv_dir_test)

trajectories= load_trajectories(matches)

#print(trajectories.keys)
combined_data=create_trajectory_dataset(matches)

train_data, val_data = split_train_val_dataset(combined_data,trajectories,val_ratio=0.05)





# print(type(train_data))
# i=0

# for image_paths, iris_coords, iris1_coords, target_points, next_points, trajectory_ids, source_files in zip(*train_data):


#     # Print the updated information
#     print(f" {i} {image_paths}, iris:{iris_coords} iris1:{iris1_coords} trajectory_id:{trajectory_ids}")

#     i += 1

# Convert updated_train_data back to the original tuple structure if needed
# #print(train_data[5])

# #print(len(train_data[5]))

# trajectories_test= load_trajectories(matches_test)
# combined_data_test=create_trajectory_dataset(matches_test)

# test_data, val_data__ = split_train_val_dataset(combined_data_test,trajectories_test,val_ratio=0.01)
# #print(test_data[5])

# #print(trajectories.keys())


from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Concatenate
from sklearn.model_selection import train_test_split
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Concatenate, Conv1D, GlobalAveragePooling1D
from tensorflow.keras.layers import Input, Conv1D, GlobalAveragePooling1D, Dense, Lambda, Concatenate
import tensorflow as tf



import matplotlib.pyplot as plt



train_traj_images=[]
train_masked_traj_images=[]
traj_ids=train_data[5]
img_path=None
imh=None
i=0
train_observer_start=[]
train_observer_end=[]
train_target_start=[]
train_target_end=[]
train_observer = np.array(train_data[1])  # iris_coords
train_target = np.array(train_data[2])  
for id in traj_ids:

    # #print(trajectories[id])
    traj=trajectories[id]
    if img_path != traj['image_path']:
        # print(traj,id)
        img_path=traj['image_path']
        dir_name=os.path.dirname(img_path)
        img_masked_path=os.path.join(dir_name,"iris1_trajectory_masked.jpg")
        print(img_masked_path)
        img_masked = tf.keras.preprocessing.image.load_img(img_masked_path, target_size=(240, 320))
        img_masked_array = tf.keras.preprocessing.image.img_to_array(img_masked)
        img_masked_array = img_masked_array / 255.0

        print(i)
        train_target_start.append(train_target[i])
        train_observer_start.append(train_observer[i])
        print(train_target[i],train_observer[i])
        if i!=0:
            train_target_end.append(train_target[i-1])
            train_observer_end.append(train_observer[i-1])
            print("end")
            print(train_target[i-1],train_observer[i-1])
        # img = tf.keras.preprocessing.image.load_img(img_path, target_size=(240, 320))
        # img_array = tf.keras.preprocessing.image.img_to_array(img)
        # img_array = img_array / 255.0




        # plt.imshow(img_array)  # Replace 0 with the index of the image you want to visualize
        # plt.axis('off')  # Turn off axis labels for better visualization
        # plt.title('Image from train_images')  # Optional title
        # # plt.show()
        # plt.imshow(img_masked_array)  # Replace 0 with the index of the image you want to visualize
        # plt.axis('off')  # Turn off axis labels for better visualization
        # plt.title('Image from train_images')  # Optional title
        # plt.show()
        #print("işlendi")
        # train_traj_images.append(img_array)



        train_masked_traj_images.append(img_masked_array)
    i+=1
print(train_target[-1],train_observer[-1])
train_target_end.append(train_target[-1])
train_observer_end.append(train_observer[-1])
save_path = '/home/ibu/bitirme_ws/traj_data_final_ekleme_npy/'  # Adjust path as needed


# train_traj_images=np.array(train_traj_images)
train_masked_traj_images=np.array(train_masked_traj_images)


print(np.shape(train_observer_start),np.shape(train_observer_end))
print(np.shape(train_target_start),np.shape(train_target_end))

train_observer_np=np.array((train_observer_start,train_observer_end))
train_target_np=np.array((train_target_start,train_target_end))
# np.save(f'{save_path}train_traj_images.npy', train_traj_images)
np.save(f'{save_path}train_masked_traj_images.npy', train_masked_traj_images)

np.save(f'{save_path}train_target_start_end.npy', train_target_np)
np.save(f'{save_path}train_observer_start_end.npy', train_observer_np)

# train_traj_images=[]
# train_masked_traj_images=[]
# #print(np.shape(train_traj_images))
# img_path=None
# imh=None
# train_images = []
# train_masked_images=[]
# train_pointed_images=[]
# train_box_centers=[]
# i=0
# for img_path in train_data[0]:
#     image_name=os.path.basename(img_path)

#     base_dir=os.path.dirname(img_path)
#     base_dir=os.path.dirname(base_dir)
#     # masked_image_path=os.path.join(base_dir,"masked_images",image_name)
#     # image_pointed_path=os.path.join(base_dir,"boxed",image_name)

#     # img_masked = tf.keras.preprocessing.image.load_img(masked_image_path, target_size=(240, 320))
#     # img_masked_array = tf.keras.preprocessing.image.img_to_array(img_masked)
#     # img_masked_array = img_masked_array / 255.0

#     # im_pointed = tf.keras.preprocessing.image.load_img(image_pointed_path, target_size=(240, 320))
#     # im_pointed_array = tf.keras.preprocessing.image.img_to_array(im_pointed)
#     # im_pointed_array = im_pointed_array / 255.0
#     img = tf.keras.preprocessing.image.load_img(img_path, target_size=(240, 320))
#     img_array = tf.keras.preprocessing.image.img_to_array(img)
#     img_array = img_array / 255.0
#     # img_array = (img_array * 255).astype(np.uint8)  # Convert back to uint8 for OpenCV

    # img=img_array
    # hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
    
    # # Create masks for both red ranges
    # lower1 = np.array([0, 19, 0])
    # upper1 = np.array([19,255, 255])
    # mask1 = cv2.inRange(hsv, lower1, upper1)

    # lower2 = np.array([156,19,0])
    # upper2 = np.array([180,255,255])
    # mask2 = cv2.inRange(hsv, lower2, upper2)
    
    # # Combine masks
    # combined_mask = cv2.bitwise_or(mask1, mask2)
    
    # # Apply masks to create results
    # result1 = cv2.bitwise_and(img, img, mask=mask1)
    # result2 = cv2.bitwise_and(img, img, mask=mask2)
    # combined_result = cv2.bitwise_and(img, img, mask=combined_mask)
    # kernel = np.ones((1,1), np.uint8)
    # combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)
    # contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # # Draw bounding boxes on original image
    # max_area = 0
    # largest_box = None
    
    # # First pass: find largest area and draw all boxes in blue
    # for contour in contours:
    #     area = cv2.contourArea(contour)
    #     x, y, w, h = cv2.boundingRect(contour)
    #     # Draw all boxes in blue
    #     # cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 1)
        
    #     # Keep track of the largest area
    #     if area > max_area:
    #         max_area = area
    #         largest_box = (x, y, w, h)
    
    # # Draw the largest box in red with thicker line
    # blank_image=np.zeros_like(img)
    # if largest_box is not None:
    #     x, y, w, h = largest_box
    #     train_box_centers.append((int(x+(w/2)),int(y+(h/2))))
    #     # cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 1)
    # else:
    #     train_box_centers((0,0))

    
    # i+=1
    # if i%10==0:

    #     plt.imshow(img)  # Replace 0 with the index of the image you want to visualize
    #     plt.axis('off')  # Turn off axis labels for better visualization
    #     plt.title('Image from trairrrrn_images')  # Optional title
    #     plt.show()
        # plt.imshow(im_pointed_array)  # Replace 0 with the index of the image you want to visualize
        # plt.axis('off')  # Turn off axis labels for better visualization
        # plt.title('Image from train_images')  # Optional title
        # plt.show()
        # plt.imshow(img_masked_array)  # Replace 0 with the index of the image you want to visualize
        # plt.axis('off')  # Turn off axis labels for better visualization
        # plt.title('Image from train_images')  # Optional title
        # plt.show()
    # train_images.append(img_array)
    # train_masked_images.append(img_masked_array)
    # train_pointed_images.append(im_pointed_array)

# train_images = np.array(train_images)
# train_masked_images = np.array(train_masked_images)
# train_pointed_images = np.array(train_pointed_images)
# train_box_centers = np.array(train_box_centers)

# train_observer = np.array(train_data[1])  # iris_coords
# train_target = np.array(train_data[2])    # iris1_coords
# train_traj_features = np.array([trajectories[traj_id]['features'] for traj_id in train_data[5]])

# #print(np.shape(train_traj_features))
# #print(np.shape(train_target))

# #print(np.shape(train_observer))
# #print(np.shape(train_images))
# #print(np.shape(train_traj_images))



# Save the processed arrays
# np.save(f'{save_path}train_box_centers.npy', train_box_centers)


# np.save(f'{save_path}train_images.npy', train_images)
# np.save(f'{save_path}train_masked_images.npy', train_masked_images)
# np.save(f'{save_path}train_drne_with_traj_images.npy', train_pointed_images)

# np.save(f'{save_path}train_observer.npy', train_observer)
# np.save(f'{save_path}train_target.npy', train_target)
# np.save(f'{save_path}train_traj_features.npy', train_traj_features)


# train_images=[]
# train_masked_images=[]
# train_pointed_images=[]
# # from tensorflow.keras import Input, Model
# # from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Concatenate
# # from sklearn.model_selection import train_test_split
# # from tensorflow.keras import Input, Model
# # from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Concatenate, Conv1D, GlobalAveragePooling1D
# # from tensorflow.keras.layers import Input, Conv1D, GlobalAveragePooling1D, Dense, Lambda, Concatenate
# # import tensorflow as tf
val_traj_images=[]
val_masked_traj_images=[]
traj_ids=val_data[5]
img_path=None
imh=None
val_observer_start=[]
val_observer_end=[]
val_target_start=[]
val_target_end=[]
val_observer = np.array(val_data[1])
val_target = np.array(val_data[2])
i=0
for id in traj_ids:

    # #print(trajectories[id])
    traj=trajectories[id]
    if img_path != traj['image_path']:
        # print(traj,id)
        img_path=traj['image_path']
        dir_name=os.path.dirname(img_path)
        img_masked_path=os.path.join(dir_name,"iris1_trajectory_masked.jpg")
        print(img_masked_path)
        img_masked = tf.keras.preprocessing.image.load_img(img_masked_path, target_size=(240, 320))
        img_masked_array = tf.keras.preprocessing.image.img_to_array(img_masked)
        img_masked_array = img_masked_array / 255.0

        print(i)
        val_target_start.append(val_target[i])
        val_observer_start.append(val_observer[i])
        print(val_target[i],val_observer[i])
        if i!=0:
            val_target_end.append(val_target[i-1])
            val_observer_end.append(val_observer[i-1])
            print("end")
            print(val_target[i-1],val_observer[i-1])
        # img = tf.keras.preprocessing.image.load_img(img_path, target_size=(240, 320))
        # img_array = tf.keras.preprocessing.image.img_to_array(img)
        # img_array = img_array / 255.0




        # plt.imshow(img_array)  # Replace 0 with the index of the image you want to visualize
        # plt.axis('off')  # Turn off axis labels for better visualization
        # plt.title('Image from train_images')  # Optional title
        # # plt.show()
        # plt.imshow(img_masked_array)  # Replace 0 with the index of the image you want to visualize
        # plt.axis('off')  # Turn off axis labels for better visualization
        # plt.title('Image from train_images')  # Optional title
        # plt.show()
        #print("işlendi")
        # train_traj_images.append(img_array)



        val_masked_traj_images.append(img_masked_array)
    i+=1
# val_traj_images=np.array(val_traj_images)
val_masked_traj_images=np.array(val_masked_traj_images)


# np.save(f'{save_path}val_traj_images.npy', val_traj_images)

np.save(f'{save_path}val_masked_traj_images.npy', val_masked_traj_images)


val_target_end.append(val_target[-1])
val_observer_end.append(val_observer[-1])


# train_traj_images=np.array(train_traj_images)


print(np.shape(val_observer_start),np.shape(val_observer_end))
print(np.shape(val_target_start),np.shape(val_target_end))

val_observer_np=np.array((val_observer_start,val_observer_end))
val_target_np=np.array((val_target_start,val_target_end))
# np.save(f'{save_path}train_traj_images.npy', train_traj_images)

np.save(f'{save_path}val_target_start_end.npy',val_target_np)
np.save(f'{save_path}val_observer_start_end.npy', val_observer_np)
# # #print(len(val_traj_images),len(val_data[0]))
# val_traj_images=[]
# val_masked_traj_images=[]

# val_images = []
# # val_masked_images=[]
# val_pointed_images=[]
# val_box_centers=[]
# for img_path in val_data[0]:


#     image_name=os.path.basename(img_path)

#     base_dir=os.path.dirname(img_path)
#     base_dir=os.path.dirname(base_dir)
#     # masked_image_path=os.path.join(base_dir,"masked_images",image_name)
#     # image_pointed_path=os.path.join(base_dir,"drone_with_trajectory_images",image_name)

#     # # img_masked = tf.keras.preprocessing.image.load_img(masked_image_path, target_size=(240, 320))
#     # # img_masked_array = tf.keras.preprocessing.image.img_to_array(img_masked)
#     # # img_masked_array = img_masked_array / 255.0

#     # im_pointed = tf.keras.preprocessing.image.load_img(image_pointed_path, target_size=(240, 320))
#     # im_pointed_array = tf.keras.preprocessing.image.img_to_array(im_pointed)
#     # im_pointed_array = im_pointed_array / 255.0




#     img = tf.keras.preprocessing.image.load_img(img_path, target_size=(240, 320))
#     img_array = tf.keras.preprocessing.image.img_to_array(img)
#     img_array = img_array / 255.0
#     img_array = (img_array * 255).astype(np.uint8)  # Convert back to uint8 for OpenCV

#     img=img_array
#     hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
    
#     # Create masks for both red ranges
#     lower1 = np.array([0, 19, 0])
#     upper1 = np.array([19,255, 255])
#     mask1 = cv2.inRange(hsv, lower1, upper1)

#     lower2 = np.array([156,19,0])
#     upper2 = np.array([180,255,255])
#     mask2 = cv2.inRange(hsv, lower2, upper2)
    
#     # Combine masks
#     combined_mask = cv2.bitwise_or(mask1, mask2)
    
#     # Apply masks to create results
#     result1 = cv2.bitwise_and(img, img, mask=mask1)
#     result2 = cv2.bitwise_and(img, img, mask=mask2)
#     combined_result = cv2.bitwise_and(img, img, mask=combined_mask)
#     kernel = np.ones((1,1), np.uint8)
#     combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)
#     contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
#     # Draw bounding boxes on original image
#     max_area = 0
#     largest_box = None
    
#     # First pass: find largest area and draw all boxes in blue
#     for contour in contours:
#         area = cv2.contourArea(contour)
#         x, y, w, h = cv2.boundingRect(contour)
#         # Draw all boxes in blue
#         # cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 1)
        
#         # Keep track of the largest area
#         if area > max_area:
#             max_area = area
#             largest_box = (x, y, w, h)
    
#     # Draw the largest box in red with thicker line
#     blank_image=np.zeros_like(img)
#     if largest_box is not None:
#         x, y, w, h = largest_box
#         val_box_centers.append((int(x+(w/2)),int(y+(h/2))))
#         # cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 1)
#     else:
#         val_box_centers.append((0,0))

    # val_images.append(img_array)
    # val_masked_images.append(img_masked_array)
    # val_pointed_images.append(im_pointed_array)

# val_images = np.array(val_images)
# # val_masked_images=np.array(val_masked_images)
# val_pointed_images=np.array(val_pointed_images)

# val_observer = np.array(val_data[1])
# val_target = np.array(val_data[2])
# val_traj_features = np.array([trajectories[traj_id]['features']  for traj_id in val_data[5]])
# val_box_centers=np.array(val_box_centers)
#print(np.shape(val_traj_images))
#print(np.shape(val_images))

#print(np.shape(val_observer))
#print(np.shape(val_target))
#print(np.shape(val_traj_features))

# np.save(f'{save_path}val_box_centers.npy', val_box_centers)

#save to path
# np.save(f'{save_path}val_images.npy', val_images)
# # np.save(f'{save_path}val_masked_images.npy', val_masked_images)
# np.save(f'{save_path}val_drone_with_traj_images.npy', val_pointed_images)
# np.save(f'{save_path}val_observer.npy', val_observer)
# np.save(f'{save_path}val_target.npy', val_target)
# np.save(f'{save_path}val_traj_features.npy', val_traj_features)


# val_images=[]

# val_masked_images=[]

# val_pointed_images=[]



# test_traj_images=[]

# # test_masked_traj_images=[]
# # traj_ids=test_data[5]
# # img_path=None
# # imh=None
# # print(trajectories_test)
# # for id in traj_ids:

# #     # #print(trajectories[id])
# #     traj=trajectories_test[id]
# #     if img_path != traj['image_path']:

# #         img_path=traj['image_path']

# #         dir_name=os.path.dirname(img_path)
# #         img_masked_path=os.path.join(dir_name,"iris1_trajectory_masked.jpg")
# #         img_masked = tf.keras.preprocessing.image.load_img(img_masked_path, target_size=(240, 320))
# #         img_masked_array = tf.keras.preprocessing.image.img_to_array(img_masked)
# #         img_masked_array = img_masked_array / 255.0


# #     #     img = tf.keras.preprocessing.image.load_img(img_path, target_size=(240, 320))
# #     #     img_array = tf.keras.preprocessing.image.img_to_array(img)
# #     #     img_array = img_array / 255.0


# #     #     #print("işlendi")
# #     # test_traj_images.append(img_array)
# #     test_masked_traj_images.append(img_masked_array)
# # test_traj_images=np.array(test_traj_images)
# # test_masked_traj_images=np.array(test_masked_traj_images)

# # # np.save(f'{save_path}test_traj_images.npy', test_traj_images)
# # # np.save(f'{save_path}test_masked_traj_images.npy', test_masked_traj_images)
# test_traj_images=[]
# test_masked_traj_images=[]
# test_images=[]
# test_masked_images=[]
# test_pointed_images=[]
# i=0
# test_box_centers=[]
# for img_path in test_data[0]:
# #     image_name=os.path.basename(img_path)

# #     base_dir=os.path.dirname(img_path)
# #     base_dir=os.path.dirname(base_dir)
# #     # masked_image_path=os.path.join(base_dir,"masked_images",image_name)
# #     image_pointed_path=os.path.join(base_dir,"drone_with_trajectory_images",image_name)

# #     # img_masked = tf.keras.preprocessing.image.load_img(masked_image_path, target_size=(240, 320))
# #     # img_masked_array = tf.keras.preprocessing.image.img_to_array(img_masked)
# #     # img_masked_array = img_masked_array / 255.0

# #     im_pointed = tf.keras.preprocessing.image.load_img(image_pointed_path, target_size=(240, 320))
# #     im_pointed_array = tf.keras.preprocessing.image.img_to_array(im_pointed)
# #     im_pointed_array = im_pointed_array / 255.0



#     img = tf.keras.preprocessing.image.load_img(img_path, target_size=(240, 320))
#     img_array = tf.keras.preprocessing.image.img_to_array(img)
#     img_array = img_array / 255.0
#     img_array = (img_array * 255).astype(np.uint8)  # Convert back to uint8 for OpenCV

#     img=img_array
#     hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
    
#     # Create masks for both red ranges
#     lower1 = np.array([0, 19, 0])
#     upper1 = np.array([19,255, 255])
#     mask1 = cv2.inRange(hsv, lower1, upper1)

#     lower2 = np.array([156,19,0])
#     upper2 = np.array([180,255,255])
#     mask2 = cv2.inRange(hsv, lower2, upper2)
    
#     # Combine masks
#     combined_mask = cv2.bitwise_or(mask1, mask2)
    
#     # Apply masks to create results
#     result1 = cv2.bitwise_and(img, img, mask=mask1)
#     result2 = cv2.bitwise_and(img, img, mask=mask2)
#     combined_result = cv2.bitwise_and(img, img, mask=combined_mask)
#     kernel = np.ones((1,1), np.uint8)
#     combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)
#     contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
#     # Draw bounding boxes on original image
#     max_area = 0
#     largest_box = None
    
#     # First pass: find largest area and draw all boxes in blue
#     for contour in contours:
#         area = cv2.contourArea(contour)
#         x, y, w, h = cv2.boundingRect(contour)
#         # Draw all boxes in blue
#         # cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 1)
        
#         # Keep track of the largest area
#         if area > max_area:
#             max_area = area
#             largest_box = (x, y, w, h)
    
#     # Draw the largest box in red with thicker line
#     blank_image=np.zeros_like(img)
#     if largest_box is not None:
#         x, y, w, h = largest_box
#         test_box_centers.append((int(x+(w/2)),int(y+(h/2))))
#         # cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 1)
#     else:
#         test_box_centers.append((0,0))



#     test_images.append(img_array)
#     # test_masked_images.append(img_masked_array)
#     test_pointed_images.append(im_pointed_array)
#     if i%20==0:
#         plt.imshow(im_pointed_array)  # Replace 0 with the index of the image you want to visualize
#         plt.axis('off')  # Turn off axis labels for better visualization
#         plt.title('Image from train_images')  # Optional title
#         plt.show()
#     i+=1
# test_images = np.array(test_images)

# test_box_centers=np.array(test_box_centers)
# # # test_masked_images=np.array(test_masked_images)
# # test_pointed_images= np.array(test_pointed_images)
# # test_observer = np.array(test_data[1])
# # test_target = np.array(test_data[2])
# # test_traj_features = np.array([trajectories_test[traj_id]['features']  for traj_id in test_data[5]])


# # #print(np.shape(test_traj_images))
# # #print(np.shape(test_images))

# # #print(np.shape(test_observer))
# # #print(np.shape(test_target))
# # #print(np.shape(test_traj_features))


# np.save(f'{save_path}test_box_centers.npy', test_box_centers)

# np.save(f'{save_path}test_images.npy', test_images)
# # np.save(f'{save_path}test_masked_images.npy', test_masked_images)
# np.save(f'{save_path}test_drone_with_traj_images.npy', test_pointed_images)

# np.save(f'{save_path}test_observer.npy', test_observer)
# np.save(f'{save_path}test_target.npy', test_target)
# np.save(f'{save_path}test_traj_features.npy', test_traj_features)



# # #cnn2
# # from tensorflow.keras import Model, Input
# # from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Concatenate, Flatten

# # # Main Image Input
# # image_input = Input(shape=(128, 128, 3), name='image_input')
# # x = Conv2D(32, (5, 5), activation='relu', padding='same')(image_input)
# # x = BatchNormalization()(x)
# # x = MaxPooling2D((2, 2))(x)

# # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
# # x = BatchNormalization()(x)
# # x = MaxPooling2D((2, 2))(x)
# # x = Dropout(0.3)(x)

# # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
# # x = BatchNormalization()(x)
# # x = MaxPooling2D((2, 2))(x)

# # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)
# # x = BatchNormalization()(x)
# # x_main = GlobalAveragePooling2D()(x)

# # # Trajectory Image Input
# # trajectory_input = Input(shape=(128, 128, 3), name='trajectory_input')
# # y = Conv2D(32, (5, 5), activation='relu', padding='same')(trajectory_input)
# # y = BatchNormalization()(y)
# # y = MaxPooling2D((2, 2))(y)

# # y = Conv2D(64, (3, 3), activation='relu', padding='same')(y)
# # y = BatchNormalization()(y)
# # y = MaxPooling2D((2, 2))(y)
# # y = Dropout(0.3)(y)

# # y = Conv2D(128, (3, 3), activation='relu', padding='same')(y)
# # y = BatchNormalization()(y)
# # y = MaxPooling2D((2, 2))(y)

# # y = Conv2D(256, (3, 3), activation='relu', padding='same')(y)
# # y = BatchNormalization()(y)
# # y_trajectory = GlobalAveragePooling2D()(y)

# # # Observer Drone Coordinates Input
# # observer_input = Input(shape=(3,), name='observer_input')

# # # Combine All Features
# # combined = Concatenate()([x_main, y_trajectory, observer_input])

# # # Fully Connected Layers
# # z = Dense(128, activation='relu')(combined)
# # z = Dropout(0.4)(z)
# # z = Dense(64, activation='relu')(z)

# # # Output Layer
# # output = Dense(3, activation='linear', name='output')(z)

# # # Model Definition
# # model = Model(inputs=[image_input, trajectory_input, observer_input], outputs=output)

# # # Compile the Model
# # model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# # # Summary
# # model.summary()


# # early_stopping = tf.keras.callbacks.EarlyStopping(
# #         monitor='val_loss',
# #         patience=150,
# #         restore_best_weights=True
# #     )

# # reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
# #         monitor='val_loss',
# #         factor=0.5,
#         patience=510,
#         min_lr=1e-6
#     )

#     # Train model
# history = model.fit(
#         [train_images, train_traj_images,train_observer],
#         train_target,
#         epochs=100,
#         batch_size=32,
#         validation_data=(
#             [val_images, val_traj_images,val_observer ],
#             val_target
#         ),
#         callbacks=[early_stopping, reduce_lr]
#     )


# model.save("/home/ibu/bitirme_ws/datas_20_30_train_npy_files/model_with_basic_cnn2_image_trajectory.keras")
# # trajectories_test= load_trajectories(matches_test)
# # combined_data_test=create_trajectory_dataset(matches_test)

# # test_data, val_data__ = split_train_val_dataset(combined_data_test,trajectories_test,val_ratio=0.01)
# # #print(test_data[5])

# test_traj_images=[]
# traj_ids=test_data[5]
# img_path=None
# imh=None
# for id in traj_ids:

#     # #print(trajectories[id])
#     traj=trajectories_test[id]
#     if img_path != traj['image_path']:

#         img_path=traj['image_path']
#         img = tf.keras.preprocessing.image.load_img(img_path, target_size=(128, 128))
#         img_array = tf.keras.preprocessing.image.img_to_array(img)
#         img_array = img_array / 255.0
#         #print("işlendi")
#     test_traj_images.append(img_array)
# test_traj_images=np.array(test_traj_images)

# save_path = '/content/drive/MyDrive/bitirme_data/'  # Adjust path as needed

# # Save the processed arrays
# test_images=np.load(f'{save_path}test_images.npy' )
# test_observer=np.load(f'{save_path}test_observer.npy')
# test_target=np.load(f'{save_path}test_target.npy' )
# test_traj_features=np.load(f'{save_path}test_traj_features.npy' )

# model=None

# #cnn_1
# from tensorflow.keras import Input, Model
# from sklearn.model_selection import train_test_split
# from tensorflow.keras import Input, Model
# from tensorflow.keras.layers import (
#     Dense,
#     Flatten,
#     Conv2D,
#     MaxPooling2D,
#     Concatenate,
#     BatchNormalization,
#     Dropout
# )
# import tensorflow as tf
# image_input = Input(shape=(128, 128, 3), name='image_input')
# x = Conv2D(32, (3,3), activation='relu', name='conv1')(image_input)
# x = MaxPooling2D(name='pool1')(x)
# x = Conv2D(64, (3,3), activation='relu', name='conv2')(x)
# x = MaxPooling2D(name='pool2')(x)
# x = Conv2D(128, (3,3), activation='relu', name='conv3')(x)
# x = MaxPooling2D(name='pool3')(x)
# image_features = Flatten()(x)

# trajectory_input = Input(shape=(128, 128, 3), name='trajectory_input')
# t = Conv2D(32, (3,3), activation='relu', padding='same')(trajectory_input)
# t = MaxPooling2D()(t)
# t = Conv2D(64, (3,3), activation='relu', padding='same')(t)
# t = MaxPooling2D()(t)
# t = Conv2D(128, (3,3), activation='relu', padding='same')(t)
# t = MaxPooling2D()(t)
# trajectory_features = Flatten()(t)
# # Observer drone position input
# observer_input = Input(shape=(3,), name='observer_input')

# # Trajectory features input
# # trajectory_features_input = Input(shape=(22,), name='trajectory_input')

# # Combine features
# combined = Concatenate()([image_features, trajectory_features, observer_input])

#     # Dense layers for final prediction
# x = Dense(512, activation='relu')(combined)
# x = BatchNormalization()(x)
# x = Dropout(0.5)(x)
# x = Dense(256, activation='relu')(x)
# x = BatchNormalization()(x)
# x = Dropout(0.5)(x)

# # Output layer predicts target drone position
# output = Dense(3, activation='linear', name='target_position')(x)

# model = Model(
#    inputs=[image_input,trajectory_input, observer_input],
#    outputs=output,
#    name='target_prediction_model'
# )
# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, clipvalue=1.0)
# model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
# model.summary()



# #cnn2
# from tensorflow.keras import Model, Input
# from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Concatenate, Flatten

# # Main Image Input
# image_input = Input(shape=(128, 128, 3), name='image_input')
# x = Conv2D(32, (5, 5), activation='relu', padding='same')(image_input)
# x = BatchNormalization()(x)
# x = MaxPooling2D((2, 2))(x)

# x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
# x = BatchNormalization()(x)
# x = MaxPooling2D((2, 2))(x)
# x = Dropout(0.3)(x)

# x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
# x = BatchNormalization()(x)
# x = MaxPooling2D((2, 2))(x)

# x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)
# x = BatchNormalization()(x)
# x_main = GlobalAveragePooling2D()(x)

# # Trajectory Image Input
# trajectory_input = Input(shape=(128, 128, 3), name='trajectory_input')
# y = Conv2D(32, (5, 5), activation='relu', padding='same')(trajectory_input)
# y = BatchNormalization()(y)
# y = MaxPooling2D((2, 2))(y)

# y = Conv2D(64, (3, 3), activation='relu', padding='same')(y)
# y = BatchNormalization()(y)
# y = MaxPooling2D((2, 2))(y)
# y = Dropout(0.3)(y)

# y = Conv2D(128, (3, 3), activation='relu', padding='same')(y)
# y = BatchNormalization()(y)
# y = MaxPooling2D((2, 2))(y)

# y = Conv2D(256, (3, 3), activation='relu', padding='same')(y)
# y = BatchNormalization()(y)
# y_trajectory = GlobalAveragePooling2D()(y)

# # Observer Drone Coordinates Input
# observer_input = Input(shape=(3,), name='observer_input')

# # Combine All Features
# combined = Concatenate()([x_main, y_trajectory, observer_input])

# # Fully Connected Layers
# z = Dense(128, activation='relu')(combined)
# z = Dropout(0.4)(z)
# z = Dense(64, activation='relu')(z)

# # Output Layer
# output = Dense(3, activation='linear', name='output')(z)

# # Model Definition
# model = Model(inputs=[image_input, trajectory_input, observer_input], outputs=output)

# # Compile the Model
# model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# # Summary
# model.summary()

# # from sklearn.preprocessing import StandardScaler

# # # Initialize StandardScaler
# # target_scaler = StandardScaler()

# # # Apply standardization (mean and std scaling)
# # train_target = target_scaler.fit_transform(train_target)
# # val_target = target_scaler.transform(val_target)
# # # test_target = target_scaler.transform(test_target)

# # # Apply the same standardization to observer data
# # train_observer = target_scaler.transform(train_observer)
# # val_observer = target_scaler.transform(val_observer)
# # test_observer = target_scaler.transform(test_observer)


# # #print("Mean of training data (target):", target_scaler.mean_)
# # #print("Standard deviation of training data (target):", target_scaler.scale_)

# #cnn2
# from tensorflow.keras import Model, Input
# from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Concatenate, Flatten

# # Main Image Input
# image_input = Input(shape=(128, 128, 3), name='image_input')
# x = Conv2D(32, (5, 5), activation='relu', padding='same')(image_input)
# x = BatchNormalization()(x)
# x = MaxPooling2D((2, 2))(x)

# x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
# x = BatchNormalization()(x)
# x = MaxPooling2D((2, 2))(x)
# x = Dropout(0.3)(x)

# x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
# x = BatchNormalization()(x)
# x = MaxPooling2D((2, 2))(x)

# x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)
# x = BatchNormalization()(x)
# x_main = GlobalAveragePooling2D()(x)

# # Trajectory Image Input
# trajectory_input = Input(shape=(128, 128, 3), name='trajectory_input')
# y = Conv2D(32, (5, 5), activation='relu', padding='same')(trajectory_input)
# y = BatchNormalization()(y)
# y = MaxPooling2D((2, 2))(y)

# y = Conv2D(64, (3, 3), activation='relu', padding='same')(y)
# y = BatchNormalization()(y)
# y = MaxPooling2D((2, 2))(y)
# y = Dropout(0.3)(y)

# y = Conv2D(128, (3, 3), activation='relu', padding='same')(y)
# y = BatchNormalization()(y)
# y = MaxPooling2D((2, 2))(y)

# y = Conv2D(256, (3, 3), activation='relu', padding='same')(y)
# y = BatchNormalization()(y)
# y_trajectory = GlobalAveragePooling2D()(y)

# # Observer Drone Coordinates Input
# observer_input = Input(shape=(3,), name='observer_input')

# # Combine All Features
# combined = Concatenate()([x_main, y_trajectory, observer_input])

# # Fully Connected Layers
# z = Dense(128, activation='relu')(combined)
# z = Dropout(0.4)(z)
# z = Dense(64, activation='relu')(z)

# # Output Layer
# output = Dense(3, activation='linear', name='output')(z)

# # Model Definition
# model = Model(inputs=[image_input, trajectory_input, observer_input], outputs=output)
# import tensorflow as tf

# def distance_loss(y_true, y_pred):
#     """
#     Custom loss function to calculate the Euclidean distance between true and predicted coordinates.
#     """
#     x_diff = tf.square(y_true[:, 0] - y_pred[:, 0])
#     y_diff = tf.square(y_true[:, 1] - y_pred[:, 1])
#     z_diff = tf.square(y_true[:, 2] - y_pred[:, 2])
#     return tf.reduce_mean(tf.sqrt(x_diff + y_diff + z_diff))

# # Compile the Model
# model.compile(optimizer='adam',  loss=lambda y_true, y_pred: distance_loss(y_true, y_pred),

#                metrics=['mae'])

# # Summary
# model.summary()



# early_stopping = tf.keras.callbacks.EarlyStopping(
#         monitor='val_loss',
#         patience=150,
#         restore_best_weights=True
#     )

# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
#         monitor='val_loss',
#         factor=0.5,
#         patience=510,
#         min_lr=1e-6
#     )

#     # Train model
# history = model.fit(
#         [train_images, train_traj_images,train_observer],
#         train_target,
#         epochs=100,
#         batch_size=32,
#         validation_data=(
#             [val_images, val_traj_images,val_observer ],
#             val_target
#         ),
#         callbacks=[early_stopping, reduce_lr]
#     )

# import matplotlib.pyplot as plt
# import numpy as np
# import pandas as pd
# path="/content/drive/MyDrive/bitirme_model_outputs/model_cnn2_distance_loss"
# predictions = model.predict([test_images, test_traj_images, test_observer])
# #test_target = target_scaler.inverse_transform(test_target)
# # Split true and predicted coordinates
# y_true_x, y_true_y, y_true_z = test_target[:,0], test_target[:,1], test_target[:,2]
# y_pred_x, y_pred_y, y_pred_z = predictions[:,0], predictions[:,1], predictions[:,2]

# # Calculate errors
# error_x = np.abs(y_true_x - y_pred_x)
# error_y = np.abs(y_true_y - y_pred_y)
# error_z = np.abs(y_true_z - y_pred_z)
# total_error = np.sqrt(error_x**2 + error_y**2 + error_z**2)

# # Calculate statistics
# max_error_x = np.max(error_x)
# mean_error_x = np.mean(error_x)
# max_error_y = np.max(error_y)
# mean_error_y = np.mean(error_y)
# max_error_z = np.max(error_z)
# mean_error_z = np.mean(error_z)
# max_total_error = np.max(total_error)
# mean_total_error = np.mean(total_error)

# # Visualize X coordinate
# plt.figure(figsize=(6, 6))
# plt.scatter(y_true_x, y_pred_x, alpha=0.7, label='Predictions')
# plt.plot([min(y_true_x), max(y_true_x)], [min(y_true_x), max(y_true_x)], 'r-', label='Reference')
# plt.xlabel('True X')
# plt.ylabel('Predicted X')
# plt.title('True vs Predicted - X Coordinate')
# plt.figtext(0.02, 0.02, f'Mean Error: {np.mean(error_x):.2f}m\nMax Error: {np.max(error_x):.2f}m')
# plt.legend()
# plt.savefig(path+'/x_axis.png')

# plt.show()


# # Y Koordinatı
# plt.figure(figsize=(6, 6))
# plt.scatter(y_true_y, y_pred_y, alpha=0.7, label='Tahminler')
# plt.plot([min(y_true_y), max(y_true_y)], [min(y_true_y), max(y_true_y)], color='red', label='Referans Çizgisi')
# plt.xlabel('Gerçek Y')
# plt.ylabel('Tahmin Edilen Y')
# plt.title('Gerçek vs Tahmin - Y Koordinatı')
# plt.figtext(0.02, 0.02,
#                 f'Average Error: {mean_error_y:.2f}m\nMax Error: {max_error_y:.2f}m',
#                 fontsize=10,
#                 bbox=dict(facecolor='white', alpha=0.8))
# plt.legend()
# plt.savefig(path+'/y_axis.png')
# plt.show()



# # Z Koordinatı
# plt.figure(figsize=(6, 6))
# plt.scatter(y_true_z, y_pred_z, alpha=0.7, label='Tahminler')
# plt.plot([min(y_true_z), max(y_true_z)], [min(y_true_z), max(y_true_z)], color='red', label='Referans Çizgisi')
# plt.xlabel('Gerçek Z')
# plt.ylabel('Tahmin Edilen Z')
# plt.title('Gerçek vs Tahmin - Z Koordinatı')
# plt.figtext(0.02, 0.02,
#                 f'Average Error: {mean_error_z:.2f}m\nMax Error: {max_error_z:.2f}m',
#                 fontsize=10,
#                 bbox=dict(facecolor='white', alpha=0.8))
# plt.legend()
# plt.savefig(path+'/z_axis.png')

# plt.show()

# model.save("/content/drive/MyDrive/bitirme_model_outputs/model_cnn2/model_with_basic_cnn2_image_trajectory.keras")

# # camera_input = Input(shape=image_shape, name='camera_input')
# #     c = Conv2D(32, (3,3), activation='relu', padding='same')(camera_input)
# #     c = MaxPooling2D()(c)
# #     c = Conv2D(64, (3,3), activation='relu', padding='same')(c)
# #     c = MaxPooling2D()(c)
# #     c = Conv2D(128, (3,3), activation='relu', padding='same')(c)
# #     c = MaxPooling2D()(c)
# #     camera_features = Flatten()(c)

# #     # Trajectory image branch
# #     trajectory_input = Input(shape=image_shape, name='trajectory_input')
# #     t = Conv2D(32, (3,3), activation='relu', padding='same')(trajectory_input)
# #     t = MaxPooling2D()(t)
# #     t = Conv2D(64, (3,3), activation='relu', padding='same')(t)
# #     t = MaxPooling2D()(t)
# #     t = Conv2D(128, (3,3), activation='relu', padding='same')(t)
# #     t = MaxPooling2D()(t)
# #     trajectory_features = Flatten()(t)

# #     # Observer drone position input
# #     observer_input = Input(shape=(3,), name='observer_input')

# #     # Combine all features
# #     combined = Concatenate()([camera_features, trajectory_features, observer_input])

# #     # Dense layers for final prediction
# #     x = Dense(512, activation='relu')(combined)
# #     x = BatchNormalization()(x)
# #     x = Dropout(0.5)(x)
# #     x = Dense(256, activation='relu')(x)
# #     x = BatchNormalization()(x)
# #     x = Dropout(0.5)(x)

# #     # Output layer predicts target drone position (x, y, z)
# #     output = Dense(3, activation='linear', name='target_position')(x)

# #     # Create model
# #     model = Model(
# #         inputs=[camera_input, trajectory_input, observer_input],
# #         outputs=output,
# #         name='triple_input_target_prediction'
# #     )

# #     # Compile model
# #     optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
# #     model.compile(
# #         optimizer=optimizer,
# #         loss='mse',
# #         metrics=['mae']
# #     )