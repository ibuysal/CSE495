# -*- coding: utf-8 -*-
"""Copy of iamge_feature.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_AyYoIOZP8zGs81vXSHJsBvS-tnvs94i

taejectory featuere as a imge from observer drone. trained and tested on 40 20 noise on same distance.
"""

# #mount drive
# from google.colab import drive
# drive.mount('/content/drive')

import os
import cv2
import numpy as np
import pandas as pd


import os

# 1. Ana klasörde "images" ile biten klasörleri bul
def find_images_dirs(base_dir):
    images_dirs = []
    for root, dirs, files in os.walk(base_dir):
        for dir_name in dirs:
            if dir_name == "images":  # Match exactly "images"
                images_dirs.append(os.path.join(root, dir_name))
    return images_dirs

# 2. Her "images" klasörüne karşılık gelen CSV dosyasını ara (isim içinde eşleşme)
def find_matching_csv(images_dirs, csv_dir):
    matches = {}

    for image_dir in images_dirs:
        # "images" kelimesini kaldır ve klasör ismini al
        path_parts = image_dir.split('/')
        print(path_parts)
    # Find the index where 'images' appears
        images_index = path_parts.index('images')

        # Get the two directories before 'images'
        experiment_path = '/'.join(path_parts[images_index-2:images_index-1])
        folder_prefix = experiment_path
        #print(folder_prefix)
        # CSV dosyalarını tara
        matched_csv = None
        for csv_file in sorted(os.listdir(csv_dir)):
            if csv_file.endswith(".csv") and folder_prefix in csv_file:
                matched_csv = os.path.join(csv_dir, csv_file)
                break  # İlk eşleşmeyi bulduktan sonra çık

        matches[image_dir] = matched_csv  # Eşleşme yoksa None olarak eklenir

    return matches

# 3. Kullanım
base_dir = "/home/ibu/bitirme_ws/datas_20_30"  # "images" klasörlerinin olduğu ana klasör
csv_dir = "/home/ibu/bitirme_ws/datas_20_30_csv"   # CSV dosyalarının olduğu klasör

# Adım 1: "images" klasörlerini bul
images_dirs = find_images_dirs(base_dir)
#print(f"Bulunan 'images' klasörleri: {images_dirs}")
#find base directory
import os
data_dir= [os.path.dirname(path) for path in images_dirs]
# Adım 2: Eşleşen CSV dosyalarını bul
matches = find_matching_csv(images_dirs, csv_dir)
#print(matches)


# 3. Kullanım
base_dir_test = "/home/ibu/bitirme_ws/test_20_30"  # "images" klasörlerinin olduğu ana klasör
csv_dir_test = "/home/ibu/bitirme_ws/test_20_30_csv"   # CSV dosyalarının olduğu klasör

# Adım 1: "images" klasörlerini bul
images_dirs_test = find_images_dirs(base_dir_test)
#print(f"Bulunan 'images' klasörleri: {images_dirs_test}")
#find base directory
import os
data_dir= [os.path.dirname(path) for path in images_dirs_test]
# Adım 2: Eşleşen CSV dosyalarını bul
matches_test = find_matching_csv(images_dirs_test, csv_dir_test)

import numpy as np
import pandas as pd
def extract_advanced_trajectory_features(trajectory):
   # Distance metrics
   total_length = np.sum(np.sqrt(np.sum(np.diff(trajectory, axis=0)**2, axis=1)))
   direct_dist = np.linalg.norm(trajectory[-1] - trajectory[0])
   sinuosity = total_length / direct_dist

   # Turning analysis
   vectors = np.diff(trajectory, axis=0)
   angles = np.arctan2(vectors[1:,1], vectors[1:,0]) - np.arctan2(vectors[:-1,1], vectors[:-1,0])
   mean_angle = np.mean(angles)
   angle_var = np.var(angles)

   # Spatial distribution
   centroid = np.mean(trajectory, axis=0)
   r_gyr = np.sqrt(np.mean(np.sum((trajectory - centroid)**2, axis=1)))
   bbox = np.max(trajectory, axis=0) - np.min(trajectory, axis=0)

   # Key points
   start = trajectory[0]
   end = trajectory[-1]
   mid = trajectory[len(trajectory)//2]
   height_var = np.max(trajectory[:,2]) - np.min(trajectory[:,2])

   features = np.concatenate([
       [total_length, direct_dist, sinuosity],  # 3
       [mean_angle, angle_var],                 # 2
       centroid,                                # 3
       [r_gyr],                                 # 1
       bbox,                                    # 3
       start, mid, end,                         # 9
       [height_var]                             # 1
   ])

   return features

def load_trajectories(matches):
    """
    Load all trajectories once and store them with unique IDs
    """
    trajectories = {}

    for image_dir, csv_path in matches.items():
        if csv_path:
            data_dir = os.path.dirname(image_dir)
            xyz_path = os.path.join(data_dir, 'iris1_XYZ.csv')
            # #print(f"xyz_path {xyz_path}")
            # Generate unique ID for this trajectory (using directory name)
            trajectory_dir_name = os.path.dirname(xyz_path)
            trajectory_id = (os.path.dirname(trajectory_dir_name))
            trajectory_id=os.path.basename(trajectory_id)
            # #print(trajectory_id)
            # Load trajectory if not already loaded
            if trajectory_id not in trajectories:
                # Read XYZ data
                trajectory_df = pd.read_csv(xyz_path)
                trajectory_df = trajectory_df.sort_values(by=['Sec', 'Nanosec']).reset_index(drop=True)
                trajectory_points = trajectory_df[['Position X', 'Position Y', 'Position Z']].values

                # Store trajectory
                traj_image_path=os.path.join(data_dir,"iris1_trajectory.jpg")
                # #print(traj_image_path)
                trajectories[trajectory_id] = {
                    'points': trajectory_points,
                    'start_time': trajectory_df.iloc[0]['Sec'],
                    'end_time': trajectory_df.iloc[-1]['Sec'],
                    'image_path': traj_image_path,
                    'features': extract_advanced_trajectory_features(trajectory_points)
                }

                #print(f"Loaded trajectory {trajectory_id}: {trajectory_points.shape} points")

    return trajectories


def find_trajectory_point_and_next(timestamp_sec, timestamp_nanosec, trajectory_df):
    """
    Find current and next point in trajectory based on timestamp

    Args:
        timestamp_sec: Current second timestamp
        timestamp_nanosec: Current nanosecond timestamp
        trajectory_df: DataFrame containing trajectory points with Sec, Nanosec columns

    Returns:
        tuple: (current_point, next_point) where each point is a numpy array [x,y,z]
    """
    # Calculate time difference for each point in seconds
    time_diff = (trajectory_df['Sec'] - timestamp_sec) + \
                (trajectory_df['Nanosec'] - timestamp_nanosec) * 1e-9

    # Find index of closest point (smallest absolute time difference)
    current_idx = abs(time_diff).argmin()

    # Get next index (if current is last point, use same point)
    next_idx = min(current_idx + 1, len(trajectory_df) - 1)

    # Get current and next points
    current_point = trajectory_df.iloc[current_idx][['Position X', 'Position Y', 'Position Z']].values
    next_point = trajectory_df.iloc[next_idx][['Position X', 'Position Y', 'Position Z']].values

    return current_point, next_point
def create_trajectory_dataset(matches):
    """
    Create dataset using both specific target points and full trajectory
    """
    image_paths = []
    iris_coords = []
    iris1_coords = []
    target_points = []
    next_points = []
    trajectory_ids = []
    source_files = []

    for image_dir, csv_path in matches.items():
        if csv_path:
            #print(f"Processing {csv_path}")
            # Read main CSV
            df = pd.read_csv(csv_path)
            df = df.sort_values(by=['image_sec', 'image_nanosec']).reset_index(drop=True)

            # Read trajectory data
            data_dir = os.path.dirname(image_dir)
            xyz_path = os.path.join(data_dir, 'iris1_XYZ.csv')
            #print(f"xyz_path {xyz_path}")

            trajectory_dir_name = os.path.dirname(xyz_path)
            trajectory_id = (os.path.dirname(trajectory_dir_name))
            trajectory_id=os.path.basename(trajectory_id)

            trajectory_df = pd.read_csv(xyz_path)
            trajectory_df = trajectory_df.sort_values(by=['Sec', 'Nanosec']).reset_index(drop=True)

            # Get full trajectory points
            full_trajectory = trajectory_df[['Position X', 'Position Y', 'Position Z']].values

            start_time_sec = trajectory_df.iloc[0]['Sec']
            end_time_sec = trajectory_df.iloc[-1]['Sec']

            for _, row in df.iterrows():
                if start_time_sec <= row['image_sec'] <= end_time_sec:
                    img_path = os.path.join(image_dir, row['image'])
                    if os.path.exists(img_path):
                        # Get current and next trajectory points
                        current_point, next_point = find_trajectory_point_and_next(
                            row['image_sec'],
                            row['image_nanosec'],
                            trajectory_df
                        )

                        # Get observer and target coordinates
                        x1, y1, z1 = row['x_iris'], row['y_iris'], -row['z_iris']
                        x2, y2, z2 = row['y_iris1']+5, row['x_iris1'], row['z_iris1']
                        # #print(img_path,csv_path,trajectory_id)

                        image_paths.append(img_path)
                        iris_coords.append((round(x1,4), round(y1,4), round(z1,4)))
                        iris1_coords.append((round(x2,4), round(y2,4), round(z2,4)))
                        target_points.append(current_point)
                        next_points.append(next_point)
                        trajectory_ids.append(trajectory_id)
                        source_files.append(csv_path)

    return image_paths, iris_coords, iris1_coords, target_points, next_points, trajectory_ids, source_files


def split_train_val_dataset(data_tuple, trajectories, val_ratio=0.4):
    """
    Split dataset into train and validation sets using trajectory IDs

    Args:
        data_tuple: Tuple of (image_paths, iris_coords, iris1_coords, target_points, next_points, trajectories, source_files)
        trajectories: Dictionary of trajectory information from load_trajectories
        val_ratio: Ratio of data to use for validation
    Returns:
        Tuple of (train_data, val_data)
    """
    # Get trajectory IDs from our loaded trajectories
    trajectory_ids = list(trajectories.keys())
    #print(f"trajectory_ids {trajectory_ids}")
    # Shuffle the trajectory IDs
    np.random.seed(42)
    np.random.shuffle(trajectory_ids)

    # Split trajectory IDs into train and validation
    val_size = int(len(trajectory_ids) * val_ratio)
    train_traj_ids = set(trajectory_ids[val_size:])
    val_traj_ids = set(trajectory_ids[:val_size])

    # #print(f"\nDataset split:")
    # #print(f"Training trajectories: {len(train_traj_ids)}")
    # #print(f"Validation trajectories: {len(val_traj_ids)}")
    # #print(f"Training trajectories: {train_traj_ids}")
    # #print(f"Validation trajectories: {val_traj_ids}")

    # # Get source files
    source_files = data_tuple[0]

    # Create masks for train and validation sets based on trajectory IDs
    train_mask = []
    val_mask = []

    for i, source_file in enumerate(source_files):
        # Extract trajectory ID from source file path

        trajectory_dir_name = os.path.dirname(source_file)
        base_dir = os.path.dirname(trajectory_dir_name)
        base_dir = os.path.dirname(base_dir)

        traj_id = os.path.basename(base_dir)
        #print(trajectory_dir_name,base_dir,traj_id)
        if traj_id in train_traj_ids:
            train_mask.append(i)
        elif traj_id in val_traj_ids:
            val_mask.append(i)

    # Function to apply mask to data
    def apply_mask(data, mask):
        if isinstance(data, (list, tuple)):
            return [data[i] for i in mask]
        return [data[i] for i in mask]

    # Split all data using masks

    train_data = tuple(apply_mask(d, train_mask) for d in data_tuple)
    val_data = tuple(apply_mask(d, val_mask) for d in data_tuple)
    # #print(train_data)
    # #print(f"Training samples: {len(train_data[0])}")
    # #print(f"Validation samples: {len(val_data[0])}")

    return train_data, val_data

trajectories= load_trajectories(matches)

#print(trajectories.keys)
combined_data=create_trajectory_dataset(matches)

train_data, val_data = split_train_val_dataset(combined_data,trajectories,val_ratio=0.3)
print(type(train_data))
i=0

for image_paths, iris_coords, iris1_coords, target_points, next_points, trajectory_ids, source_files in zip(*train_data):


    # Print the updated information
    print(f" {i} {image_paths}, iris:{iris_coords} iris1:{iris1_coords} trajectory_id:{trajectory_ids}")

    i += 1

# Convert updated_train_data back to the original tuple structure if needed
# #print(train_data[5])

# #print(len(train_data[5]))

trajectories_test= load_trajectories(matches_test)
combined_data_test=create_trajectory_dataset(matches_test)

test_data, val_data__ = split_train_val_dataset(combined_data_test,trajectories_test,val_ratio=0.01)
# #print(test_data[5])

# #print(trajectories.keys())

# #print(train_data[5])
def mask_red_color(image):
    """
    Masks the red color in the input image and returns the masked image.
    """
    # Convert the image to HSV color space
    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)  # Use RGB since the image is loaded with tf
    # Define lower and upper bounds for red in HSV
    lower1 = np.array([0, 30, 0])
    upper1 = np.array([19,255, 255])
    mask1 = cv2.inRange(hsv, lower1, upper1)

    lower2 = np.array([156,30,0])
    upper2 = np.array([180,255,255])
    mask2 = cv2.inRange(hsv, lower2, upper2)

    # Combine masks
    combined_mask = cv2.bitwise_or(mask1, mask2)

    # Apply the mask to the original image
    masked_image = cv2.bitwise_and(image, image, mask=combined_mask)
    kernel = np.ones((1,1), np.uint8)
    masked_image = cv2.morphologyEx(masked_image, cv2.MORPH_OPEN, kernel)

    return masked_image
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Concatenate
from sklearn.model_selection import train_test_split
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Concatenate, Conv1D, GlobalAveragePooling1D
from tensorflow.keras.layers import Input, Conv1D, GlobalAveragePooling1D, Dense, Lambda, Concatenate
import tensorflow as tf



import matplotlib.pyplot as plt



# train_traj_images=[]
# train_masked_traj_images=[]
# traj_ids=train_data[5]
# img_path=None
# imh=None
# i=0
# for id in traj_ids:

#     # #print(trajectories[id])
#     traj=trajectories[id]
#     if img_path != traj['image_path']:
#         # print(traj,id)
#         img_path=traj['image_path']
#         dir_name=os.path.dirname(img_path)
#         img_masked_path=os.path.join(dir_name,"iris1_trajectory_masked.jpg")
#         img_masked = tf.keras.preprocessing.image.load_img(img_masked_path, target_size=(240, 320))
#         img_masked_array = tf.keras.preprocessing.image.img_to_array(img_masked)
#         img_masked_array = img_masked_array / 255.0


#         img = tf.keras.preprocessing.image.load_img(img_path, target_size=(240, 320))
#         img_array = tf.keras.preprocessing.image.img_to_array(img)
#         img_array = img_array / 255.0




#         plt.imshow(img_array)  # Replace 0 with the index of the image you want to visualize
#         plt.axis('off')  # Turn off axis labels for better visualization
#         plt.title('Image from train_images')  # Optional title
#         plt.show()
#         plt.imshow(img_masked_array)  # Replace 0 with the index of the image you want to visualize
#         plt.axis('off')  # Turn off axis labels for better visualization
#         plt.title('Image from train_images')  # Optional title
#         plt.show()
#         #print("işlendi")
        
#         train_masked_traj_images.append(img_masked_array)


save_path = '/home/ibu/bitirme_ws/kust_traj_npy/'  # Adjust path as needed


# train_traj_images=np.array(train_traj_images)
# train_masked_traj_images=np.array(train_masked_traj_images)
# np.save(f'{save_path}train_masked_traj_images.npy', train_masked_traj_images)


# train_traj_images=[]
# train_masked_traj_images=[]
# #print(np.shape(train_traj_images))
# img_path=None
# imh=None
# train_images = []
# train_masked_images=[]
# train_pointed_images=[]
# train_box_centers=[]
# i=0
# for img_path in train_data[0]:
#     image_name=os.path.basename(img_path)

#     base_dir=os.path.dirname(img_path)
#     base_dir=os.path.dirname(base_dir)
#     # masked_image_path=os.path.join(base_dir,"masked_images",image_name)
#     # image_pointed_path=os.path.join(base_dir,"boxed",image_name)

#     # img_masked = tf.keras.preprocessing.image.load_img(masked_image_path, target_size=(240, 320))
#     # img_masked_array = tf.keras.preprocessing.image.img_to_array(img_masked)
#     # img_masked_array = img_masked_array / 255.0

#     # im_pointed = tf.keras.preprocessing.image.load_img(image_pointed_path, target_size=(240, 320))
#     # im_pointed_array = tf.keras.preprocessing.image.img_to_array(im_pointed)
#     # im_pointed_array = im_pointed_array / 255.0
#     img = tf.keras.preprocessing.image.load_img(img_path, target_size=(240, 320))
#     img_array = tf.keras.preprocessing.image.img_to_array(img)
#     img_array = img_array / 255.0
#     # img_array = (img_array * 255).astype(np.uint8)  # Convert back to uint8 for OpenCV

#     # img=img_array
#     # hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
    
#     # # Create masks for both red ranges
#     # lower1 = np.array([0, 19, 0])
#     # upper1 = np.array([19,255, 255])
#     # mask1 = cv2.inRange(hsv, lower1, upper1)

#     # lower2 = np.array([156,19,0])
#     # upper2 = np.array([180,255,255])
#     # mask2 = cv2.inRange(hsv, lower2, upper2)
    
#     # # Combine masks
#     # combined_mask = cv2.bitwise_or(mask1, mask2)
    
#     # # Apply masks to create results
#     # result1 = cv2.bitwise_and(img, img, mask=mask1)
#     # result2 = cv2.bitwise_and(img, img, mask=mask2)
#     # combined_result = cv2.bitwise_and(img, img, mask=combined_mask)
#     # kernel = np.ones((1,1), np.uint8)
#     # combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)
#     # contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
#     # # Draw bounding boxes on original image
#     # max_area = 0
#     # largest_box = None
    
#     # # First pass: find largest area and draw all boxes in blue
#     # for contour in contours:
#     #     area = cv2.contourArea(contour)
#     #     x, y, w, h = cv2.boundingRect(contour)
#     #     # Draw all boxes in blue
#     #     # cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 1)
        
#     #     # Keep track of the largest area
#     #     if area > max_area:
#     #         max_area = area
#     #         largest_box = (x, y, w, h)
    
#     # # Draw the largest box in red with thicker line
#     # blank_image=np.zeros_like(img)
#     # if largest_box is not None:
#     #     x, y, w, h = largest_box
#     #     train_box_centers.append((int(x+(w/2)),int(y+(h/2))))
#     #     # cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 1)
#     # else:
#     #     train_box_centers((0,0))

    
#     i+=1
#     # if i%10==0:

#     #     plt.imshow(img)  # Replace 0 with the index of the image you want to visualize
#     #     plt.axis('off')  # Turn off axis labels for better visualization
#     #     plt.title('Image from trairrrrn_images')  # Optional title
#     #     plt.show()
#         # plt.imshow(im_pointed_array)  # Replace 0 with the index of the image you want to visualize
#         # plt.axis('off')  # Turn off axis labels for better visualization
#         # plt.title('Image from train_images')  # Optional title
#         # plt.show()
#         # plt.imshow(img_masked_array)  # Replace 0 with the index of the image you want to visualize
#         # plt.axis('off')  # Turn off axis labels for better visualization
#         # plt.title('Image from train_images')  # Optional title
#         # plt.show()
#     # train_images.append(img_array)
#     # train_masked_images.append(img_masked_array)
#     # train_pointed_images.append(im_pointed_array)

# # train_images = np.array(train_images)
# # train_masked_images = np.array(train_masked_images)
# # train_pointed_images = np.array(train_pointed_images)
# # train_box_centers = np.array(train_box_centers)

# # train_observer = np.array(train_data[1])  # iris_coords
# # train_target = np.array(train_data[2])    # iris1_coords
# # train_traj_features = np.array([trajectories[traj_id]['features'] for traj_id in train_data[5]])

# # #print(np.shape(train_traj_features))
# # #print(np.shape(train_target))

# # #print(np.shape(train_observer))
# # #print(np.shape(train_images))
# # #print(np.shape(train_traj_images))



# # Save the processed arrays
# # np.save(f'{save_path}train_box_centers.npy', train_box_centers)


# # np.save(f'{save_path}train_images.npy', train_images)
# # np.save(f'{save_path}train_masked_images.npy', train_masked_images)
# # np.save(f'{save_path}train_drne_with_traj_images.npy', train_pointed_images)

# # np.save(f'{save_path}train_observer.npy', train_observer)
# # np.save(f'{save_path}train_target.npy', train_target)
# # np.save(f'{save_path}train_traj_features.npy', train_traj_features)


train_images=[]
train_masked_images=[]
train_pointed_images=[]
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Concatenate
from sklearn.model_selection import train_test_split
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Concatenate, Conv1D, GlobalAveragePooling1D
from tensorflow.keras.layers import Input, Conv1D, GlobalAveragePooling1D, Dense, Lambda, Concatenate
import tensorflow as tf
val_traj_images=[]
val_masked_traj_images=[]
traj_ids=val_data[5]
img_path=None
imh=None
for id in traj_ids:

    # #print(trajectories[id])
    traj=trajectories[id]
    if img_path != traj['image_path']:

        img_path=traj['image_path']
        dir_name=os.path.dirname(img_path)
        img_masked_path=os.path.join(dir_name,"iris1_trajectory_masked.jpg")
        img_masked = tf.keras.preprocessing.image.load_img(img_masked_path, target_size=(240, 320))
        img_masked_array = tf.keras.preprocessing.image.img_to_array(img_masked)
        img_masked_array = img_masked_array / 255.0


        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(240, 320))
        img_array = tf.keras.preprocessing.image.img_to_array(img)
        img_array = img_array / 255.0
        #print("işlendi")
    # val_traj_images.append(img_array)
        val_masked_traj_images.append(img_masked_array)
# val_traj_images=np.array(val_traj_images)
val_masked_traj_images=np.array(val_masked_traj_images)


# np.save(f'{save_path}val_traj_images.npy', val_traj_images)

np.save(f'{save_path}val_masked_traj_images.npy', val_masked_traj_images)
# # # #print(len(val_traj_images),len(val_data[0]))
# # val_traj_images=[]
# # val_masked_traj_images=[]

# # val_images = []
# # # val_masked_images=[]
# # val_pointed_images=[]
# # val_box_centers=[]
# # for img_path in val_data[0]:


# #     image_name=os.path.basename(img_path)

# #     base_dir=os.path.dirname(img_path)
# #     base_dir=os.path.dirname(base_dir)
# #     # masked_image_path=os.path.join(base_dir,"masked_images",image_name)
# #     # image_pointed_path=os.path.join(base_dir,"drone_with_trajectory_images",image_name)

# #     # # img_masked = tf.keras.preprocessing.image.load_img(masked_image_path, target_size=(240, 320))
# #     # # img_masked_array = tf.keras.preprocessing.image.img_to_array(img_masked)
# #     # # img_masked_array = img_masked_array / 255.0

# #     # im_pointed = tf.keras.preprocessing.image.load_img(image_pointed_path, target_size=(240, 320))
# #     # im_pointed_array = tf.keras.preprocessing.image.img_to_array(im_pointed)
# #     # im_pointed_array = im_pointed_array / 255.0




# #     img = tf.keras.preprocessing.image.load_img(img_path, target_size=(240, 320))
# #     img_array = tf.keras.preprocessing.image.img_to_array(img)
# #     img_array = img_array / 255.0
# #     img_array = (img_array * 255).astype(np.uint8)  # Convert back to uint8 for OpenCV

# #     img=img_array
# #     hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
    
# #     # Create masks for both red ranges
# #     lower1 = np.array([0, 19, 0])
# #     upper1 = np.array([19,255, 255])
# #     mask1 = cv2.inRange(hsv, lower1, upper1)

# #     lower2 = np.array([156,19,0])
# #     upper2 = np.array([180,255,255])
# #     mask2 = cv2.inRange(hsv, lower2, upper2)
    
# #     # Combine masks
# #     combined_mask = cv2.bitwise_or(mask1, mask2)
    
# #     # Apply masks to create results
# #     result1 = cv2.bitwise_and(img, img, mask=mask1)
# #     result2 = cv2.bitwise_and(img, img, mask=mask2)
# #     combined_result = cv2.bitwise_and(img, img, mask=combined_mask)
# #     kernel = np.ones((1,1), np.uint8)
# #     combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)
# #     contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
# #     # Draw bounding boxes on original image
# #     max_area = 0
# #     largest_box = None
    
# #     # First pass: find largest area and draw all boxes in blue
# #     for contour in contours:
# #         area = cv2.contourArea(contour)
# #         x, y, w, h = cv2.boundingRect(contour)
# #         # Draw all boxes in blue
# #         # cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 1)
        
# #         # Keep track of the largest area
# #         if area > max_area:
# #             max_area = area
# #             largest_box = (x, y, w, h)
    
# #     # Draw the largest box in red with thicker line
# #     blank_image=np.zeros_like(img)
# #     if largest_box is not None:
# #         x, y, w, h = largest_box
# #         val_box_centers.append((int(x+(w/2)),int(y+(h/2))))
# #         # cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 1)
# #     else:
# #         val_box_centers.append((0,0))

#     # val_images.append(img_array)
#     # val_masked_images.append(img_masked_array)
#     # val_pointed_images.append(im_pointed_array)

# # val_images = np.array(val_images)
# # # val_masked_images=np.array(val_masked_images)
# # val_pointed_images=np.array(val_pointed_images)

# # val_observer = np.array(val_data[1])
# # val_target = np.array(val_data[2])
# # val_traj_features = np.array([trajectories[traj_id]['features']  for traj_id in val_data[5]])
# # val_box_centers=np.array(val_box_centers)
# #print(np.shape(val_traj_images))
# #print(np.shape(val_images))

# #print(np.shape(val_observer))
# #print(np.shape(val_target))
# #print(np.shape(val_traj_features))

# # np.save(f'{save_path}val_box_centers.npy', val_box_centers)

# #save to path
# # np.save(f'{save_path}val_images.npy', val_images)
# # # np.save(f'{save_path}val_masked_images.npy', val_masked_images)
# # np.save(f'{save_path}val_drone_with_traj_images.npy', val_pointed_images)
# # np.save(f'{save_path}val_observer.npy', val_observer)
# # np.save(f'{save_path}val_target.npy', val_target)
# # np.save(f'{save_path}val_traj_features.npy', val_traj_features)


# # val_images=[]

# # val_masked_images=[]

# # val_pointed_images=[]



# # test_traj_images=[]

# # # test_masked_traj_images=[]
# # # traj_ids=test_data[5]
# # # img_path=None
# # # imh=None
# # # print(trajectories_test)
# # # for id in traj_ids:

# # #     # #print(trajectories[id])
# # #     traj=trajectories_test[id]
# # #     if img_path != traj['image_path']:

# # #         img_path=traj['image_path']

# # #         dir_name=os.path.dirname(img_path)
# # #         img_masked_path=os.path.join(dir_name,"iris1_trajectory_masked.jpg")
# # #         img_masked = tf.keras.preprocessing.image.load_img(img_masked_path, target_size=(240, 320))
# # #         img_masked_array = tf.keras.preprocessing.image.img_to_array(img_masked)
# # #         img_masked_array = img_masked_array / 255.0


# # #     #     img = tf.keras.preprocessing.image.load_img(img_path, target_size=(240, 320))
# # #     #     img_array = tf.keras.preprocessing.image.img_to_array(img)
# # #     #     img_array = img_array / 255.0


# # #     #     #print("işlendi")
# # #     # test_traj_images.append(img_array)
# # #     test_masked_traj_images.append(img_masked_array)
# # # test_traj_images=np.array(test_traj_images)
# # # test_masked_traj_images=np.array(test_masked_traj_images)

# # # # np.save(f'{save_path}test_traj_images.npy', test_traj_images)
# # # np.save(f'{save_path}test_masked_traj_images.npy', test_masked_traj_images)
# test_traj_images=[]
# test_masked_traj_images=[]
# test_images=[]
# test_masked_images=[]
# test_pointed_images=[]
# i=0
# test_box_centers=[]
# for img_path in test_data[0]:
# #     image_name=os.path.basename(img_path)

# #     base_dir=os.path.dirname(img_path)
# #     base_dir=os.path.dirname(base_dir)
# #     # masked_image_path=os.path.join(base_dir,"masked_images",image_name)
# #     image_pointed_path=os.path.join(base_dir,"drone_with_trajectory_images",image_name)

# #     # img_masked = tf.keras.preprocessing.image.load_img(masked_image_path, target_size=(240, 320))
# #     # img_masked_array = tf.keras.preprocessing.image.img_to_array(img_masked)
# #     # img_masked_array = img_masked_array / 255.0

# #     im_pointed = tf.keras.preprocessing.image.load_img(image_pointed_path, target_size=(240, 320))
# #     im_pointed_array = tf.keras.preprocessing.image.img_to_array(im_pointed)
# #     im_pointed_array = im_pointed_array / 255.0



#     img = tf.keras.preprocessing.image.load_img(img_path, target_size=(240, 320))
#     img_array = tf.keras.preprocessing.image.img_to_array(img)
#     img_array = img_array / 255.0
#     img_array = (img_array * 255).astype(np.uint8)  # Convert back to uint8 for OpenCV

#     img=img_array
#     hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
    
#     # Create masks for both red ranges
#     lower1 = np.array([0, 19, 0])
#     upper1 = np.array([19,255, 255])
#     mask1 = cv2.inRange(hsv, lower1, upper1)

#     lower2 = np.array([156,19,0])
#     upper2 = np.array([180,255,255])
#     mask2 = cv2.inRange(hsv, lower2, upper2)
    
#     # Combine masks
#     combined_mask = cv2.bitwise_or(mask1, mask2)
    
#     # Apply masks to create results
#     result1 = cv2.bitwise_and(img, img, mask=mask1)
#     result2 = cv2.bitwise_and(img, img, mask=mask2)
#     combined_result = cv2.bitwise_and(img, img, mask=combined_mask)
#     kernel = np.ones((1,1), np.uint8)
#     combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)
#     contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
#     # Draw bounding boxes on original image
#     max_area = 0
#     largest_box = None
    
#     # First pass: find largest area and draw all boxes in blue
#     for contour in contours:
#         area = cv2.contourArea(contour)
#         x, y, w, h = cv2.boundingRect(contour)
#         # Draw all boxes in blue
#         # cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 1)
        
#         # Keep track of the largest area
#         if area > max_area:
#             max_area = area
#             largest_box = (x, y, w, h)
    
#     # Draw the largest box in red with thicker line
#     blank_image=np.zeros_like(img)
#     if largest_box is not None:
#         x, y, w, h = largest_box
#         test_box_centers.append((int(x+(w/2)),int(y+(h/2))))
#         # cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 1)
#     else:
#         test_box_centers.append((0,0))



# #     test_images.append(img_array)
# #     # test_masked_images.append(img_masked_array)
# #     test_pointed_images.append(im_pointed_array)
# #     if i%20==0:
# #         plt.imshow(im_pointed_array)  # Replace 0 with the index of the image you want to visualize
# #         plt.axis('off')  # Turn off axis labels for better visualization
# #         plt.title('Image from train_images')  # Optional title
# #         plt.show()
# #     i+=1
# # test_images = np.array(test_images)

# test_box_centers=np.array(test_box_centers)
# # # test_masked_images=np.array(test_masked_images)
# # test_pointed_images= np.array(test_pointed_images)
# # test_observer = np.array(test_data[1])
# # test_target = np.array(test_data[2])
# # test_traj_features = np.array([trajectories_test[traj_id]['features']  for traj_id in test_data[5]])


# # #print(np.shape(test_traj_images))
# # #print(np.shape(test_images))

# # #print(np.shape(test_observer))
# # #print(np.shape(test_target))
# # #print(np.shape(test_traj_features))


# np.save(f'{save_path}test_box_centers.npy', test_box_centers)

# # np.save(f'{save_path}test_images.npy', test_images)
# # # np.save(f'{save_path}test_masked_images.npy', test_masked_images)
# # np.save(f'{save_path}test_drone_with_traj_images.npy', test_pointed_images)

# # np.save(f'{save_path}test_observer.npy', test_observer)
# # np.save(f'{save_path}test_target.npy', test_target)
# # np.save(f'{save_path}test_traj_features.npy', test_traj_features)


