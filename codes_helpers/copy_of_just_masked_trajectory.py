# -*- coding: utf-8 -*-
"""Copy of just_masked_trajectory.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cxZH_bhRIY6uz1m9uQsUIi6o_5imT5Sf
"""

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
import numpy as np

import numpy as np
save_path = '/content/drive/MyDrive/just_traj_np/'  # Adjust path as needed

# Save the processed arrays
# train_traj_images=np.load(f'{save_path}train_traj_images.npy')
# train_masked_images= np.load(f'{save_path}train_traj_images.npy')

# train_drone_with_traj_images=np.load(f'{save_path}train_drone_with_traj_images.npy')
train_images=np.load(f'{save_path}train_masked_traj_images.npy')
# train_pointed_images=np.load(f'{save_path}train_pointed_images.npy')

train_observer=np.load(f'{save_path}train_observer_start_end.npy')
train_target=np.load(f'{save_path}train_target_start_end.npy')
train_observer = np.transpose(train_observer, (1, 0, 2))
train_target = np.transpose(train_target, (1, 0, 2))

# train_traj_features=np.load(f'{save_path}train_traj_features.npy' )
i=0
# for image in train_drone_with_traj_images:
#   if i%100==0:
#         plt.imshow(image)  # Replace 0 with the index of the image you want to visualize
#         plt.axis('off')  # Turn off axis labels for better visualization
#         plt.title('Image from train_images')  # Optional title
#         plt.show()
#   i+=1
print(np.shape(train_images))
print(np.shape(train_observer))
print(np.shape(train_target))
# print(np.shape(train_traj_features)

#print target
print(train_observer)

unique_features, indices = np.unique(train_traj_features, axis=0, return_index=True)

# Sort the indices in ascending order
start_indices = np.sort(indices)

# Print the start indices of each unique feature group
print("Start indices of each unique group:", start_indices)

train_traj_observer=[]
train_traj_target=[]

for i in range(len(start_indices)):
    start = start_indices[i]
    # Determine the end index (exclusive)
    end = start_indices[i + 1] if i + 1 < len(start_indices) else len(train_observer)
    print(end)

    train_traj_observer.append((train_observer[start],train_observer[end-1]))
    train_traj_target.append((train_target[start],train_target[end-1]))

train_traj_observer=np.array(train_traj_observer)
train_traj_target=np.array(train_traj_target)

from matplotlib import pyplot as plt
for i in range(len(start_indices)):
    start = start_indices[i]
    # Determine the end index (exclusive)
    end = start_indices[i + 1] if i + 1 < len(start_indices) else len(train_observer)
    print(end)
    start_image=train_images[start]
    end_image=train_images[end-1]
    plt.imshow(start_image)  # Replace 0 with the index of the image you want to visualize
    plt.axis('off')  # Turn off axis labels for better visualization
    plt.title('Image from train_images')  # Optional title
    plt.show()
    plt.imshow(end_image)
    plt.axis('off')  # Turn off axis labels for better visualization
    plt.title('Image from train_images')  # Optional title
    plt.show()

    print(train_observer[start],train_observer[end-1])
    print(train_target[start],train_target[end-1])

observer_start=train_observer[:,0]
observer_end=train_observer[:,1]
target_start=train_target[:,0]
target_end=train_target[:,1]

train_relative_start=observer_start-target_start
train_relative_end=observer_end-target_end

print(train_relative_start)
print(train_relative_end  )

print(np.shape(traj_observer))
print(np.shape(traj_target))

train_box_centers=np.load(f'{save_path}train_box_centers.npy')

print(np.shape(train_images))
print(np.shape(train_observer))
print(np.shape(train_target))

# Save the processed arrays
# val_drone_with_traj_images=np.load(f'{save_path}val_drone_with_traj_images.npy')

val_images=np.load(f'{save_path}val_masked_traj_images.npy')
# val_pointed_images=np.load(f'{save_path}val_pointed_images.npy')

val_observer=np.load(f'{save_path}val_observer_start_end.npy')
val_target=np.load(f'{save_path}val_target_start_end.npy')
val_observer = np.transpose(val_observer, (1, 0, 2))
val_target = np.transpose(val_target, (1, 0, 2))

print(np.shape(val_images))
print(np.shape(val_images))
print(np.shape(val_observer))

unique_features_val, indices_val = np.unique(val_traj_features, axis=0, return_index=True)

# Sort the indices in ascending order
start_indices_val = np.sort(indices_val)
print(start_indices_val)
# Print the start indices of each unique feature group

val_traj_observer=[]
val_traj_target=[]
for i in range(len(start_indices_val)):
    start = start_indices_val[i]
    # Determine the end index (exclusive)
    end = start_indices_val[i + 1] if i + 1 < len(start_indices_val) else len(val_observer)
    print(end)

    val_traj_observer.append((val_observer[start],val_observer[end-1]))
    val_traj_target.append((val_target[start],val_target[end-1]))

val_traj_observer=np.array(val_traj_observer)
val_traj_target=np.array(val_traj_target)

observer_start=val_observer[:,0]
observer_end=val_observer[:,1]
target_start=val_target[:,0]
target_end=val_target[:,1]

val_relative_start=observer_start-target_start
val_relative_end=observer_end-target_end

save_path1 = '/content/drive/MyDrive/just_traj_np_ekleme/'  # Adjust path as needed

# Save the processed arrays
# train_traj_images=np.load(f'{save_path}train_traj_images.npy')
# train_masked_images= np.load(f'{save_path}train_traj_images.npy')

# train_drone_with_traj_images=np.load(f'{save_path}train_drone_with_traj_images.npy')
train_images_ekleme=np.load(f'{save_path1}train_masked_traj_images.npy')
# train_pointed_images=np.load(f'{save_path}train_pointed_images.npy')

train_observer_ekleme=np.load(f'{save_path1}train_observer_start_end.npy')
train_target_ekleme=np.load(f'{save_path1}train_target_start_end.npy')
print(np.shape(train_images_ekleme))
print(np.shape(train_observer_ekleme))
print(np.shape(train_target_ekleme))
train_observer_ekleme = np.transpose(train_observer_ekleme, (1, 0, 2))
train_target_ekleme = np.transpose(train_target_ekleme, (1, 0, 2))

# train_traj_features=np.load(f'{save_path}train_traj_features.npy' )
i=0
# for image in train_drone_with_traj_images:
#   if i%100==0:
#         plt.imshow(image)  # Replace 0 with the index of the image you want to visualize
#         plt.axis('off')  # Turn off axis labels for better visualization
#         plt.title('Image from train_images')  # Optional title
#         plt.show()
#   i+=1
print(np.shape(train_images_ekleme))
print(np.shape(train_observer_ekleme))
print(np.shape(train_target_ekleme))

train_ekleme_relative_start=train_observer_ekleme[:,0]-train_target_ekleme[:,0]
train_ekleme_relative_end=train_observer_ekleme[:,1]-train_target_ekleme[:,1]

num_to_transfer = 12  # Number of samples to transfer from val to train
indices = np.random.choice(val_images.shape[0], size=num_to_transfer, replace=False)

val_images_subset = val_images[indices]  # Selected data to transfer
val_relative_end_subset = val_relative_end[indices]  # Selected data to transfer
val_relative_start_subset = val_relative_start[indices]  # Selected data to transfer
  # Selected data to transfer
# Step 2: Append the subset to the training data
train_images = np.vstack((train_images, val_images_subset))
train_relative_start = np.vstack((train_relative_start, val_relative_start_subset))
train_relative_end = np.vstack((train_relative_end, val_relative_end_subset))

# Step 3: Remove the transferred data from validation set
val_images= np.delete(val_images, indices, axis=0)
val_relative_start= np.delete(val_relative_start, indices, axis=0)
val_relative_end= np.delete(val_relative_end, indices, axis=0)

num_to_transfer = 6 # Number of samples to transfer from val to train
indices = np.random.choice(train_images_ekleme.shape[0], size=num_to_transfer, replace=False)

val_images_subset = train_images_ekleme[indices]  # Selected data to transfer
val_relative_end_subset = train_ekleme_relative_end[indices]  # Selected data to transfer
val_relative_start_subset = train_ekleme_relative_start[indices]  # Selected data to transfer
  # Selected data to transfer
# Step 2: Append the subset to the training data
train_images = np.vstack((train_images, val_images_subset))
train_relative_start = np.vstack((train_relative_start, val_relative_start_subset))
train_relative_end = np.vstack((train_relative_end, val_relative_end_subset))

# Step 3: Remove the transferred data from validation set
train_images_ekleme= np.delete(train_images_ekleme, indices, axis=0)
train_ekleme_relative_end= np.delete(train_ekleme_relative_end, indices, axis=0)
train_ekleme_relative_start= np.delete(train_ekleme_relative_start, indices, axis=0)

print(np.shape(train_images))
print(np.shape(train_relative_start))
print(np.shape(train_relative_end))
print(np.shape(val_images))
print(np.shape(val_relative_start))
print(np.shape(val_relative_end)  )

print(np.shape(train_images_ekleme))
print(np.shape(train_ekleme_relative_start))
print(np.shape(train_ekleme_relative_end))

for i in range(len(start_indices_val)):
    start = start_indices_val[i]
    # Determine the end index (exclusive)
    end = start_indices_val[i + 1] if i + 1 < len(start_indices_val) else len(start_indices_val)
    print(end)
    start_image=val_images[start]
    end_image=val_images[end-1]
    plt.imshow(start_image)  # Replace 0 with the index of the image you want to visualize
    plt.axis('off')  # Turn off axis labels for better visualization
    plt.title('Image from train_images')  # Optional title
    plt.show()
    plt.imshow(end_image)
    plt.axis('off')  # Turn off axis labels for better visualization
    plt.title('Image from train_images')  # Optional title
    plt.show()

    print(val_observer[start],val_observer[end-1])
    print(val_target[start],val_target[end-1])

save_path = '/content/drive/MyDrive/just_trajectory/'  # Adjust path as needed

# Save the processed arrays
# train_traj_images=np.load(f'{save_path}train_traj_images.npy')
# train_masked_images= np.load(f'{save_path}train_traj_images.npy')

train_masked_traj_images=np.load(f'{save_path}train_masked_traj_images.npy')
val_masked_traj_images=np.load(f'{save_path}val_masked_traj_images.npy')

import tensorflow as tf
from tensorflow.keras import layers, models, Input
from tensorflow.keras.applications import MobileNetV2

def create_mobilenetv2_backbone(input_shape=(240, 320, 3), trainable=False):
    """
    Creates a MobileNetV2 backbone for feature extraction with custom input shape.
    """
    base_model = MobileNetV2(
        include_top=False,
        weights='imagenet',
        input_shape=input_shape
    )
    base_model.trainable = trainable

    # e.g. output shape ~ (None, 8, 10, 1280) for (240,320) input
    x = base_model.output
    x = layers.GlobalAveragePooling2D()(x)  # -> (None, 1280)

    backbone = models.Model(inputs=base_model.input, outputs=x,
                            name="MobileNetV2_Backbone")
    return backbone


def create_model_with_init_location(
    img_shape=(240, 320, 3),
    loc_shape=(3,),
    trainable=False
):
    """
    Full model that:
      1) Accepts a trajectory image (masked with red points).
      2) Accepts the initial location (x0, y0, z0).
      3) Predicts the final location (x_end, y_end, z_end).

    If you want to predict the delta from the initial location,
    just interpret the output accordingly.
    """
    # ---------------------------
    # 1) Define inputs
    # ---------------------------
    image_input = Input(shape=img_shape, name="trajectory_image")
    init_loc_input = Input(shape=loc_shape, name="initial_location")

    # ---------------------------
    # 2) Create backbone for image
    # ---------------------------
    backbone = create_mobilenetv2_backbone(img_shape, trainable=trainable)
    image_features = backbone(image_input)  # shape (None, 1280)

    # ---------------------------
    # 3) Concatenate with init loc
    # ---------------------------
    # shape = (None, 1280 + 3)
    combined = layers.Concatenate()([image_features, init_loc_input])

    # ---------------------------
    # 4) Regression head
    # ---------------------------
    x = layers.Dense(128, activation='relu')(combined)
    x = layers.Dropout(0.3)(x)
    x = layers.Dense(32, activation='relu')(x)
    x = layers.Dropout(0.3)(x)

    # Predict final location (x_end, y_end, z_end)
    # or the delta relative to initial. Your choice.
    outputs = layers.Dense(3, activation='linear', name="end_location")(x)

    # ---------------------------
    # 5) Build model
    # ---------------------------
    model = models.Model(inputs=[image_input, init_loc_input],
                         outputs=outputs,
                         name="DroneLocationModel_MobileNetV2")
    return model



model = create_model_with_init_location((240, 320, 3), (3,), trainable=False)
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.summary()

# Suppose:
#   X_images -> shape (N, 240, 320, 3)   your masked images
#   X_initloc -> shape (N, 3)           initial (x0, y0, z0)
#   Y_endloc -> shape (N, 3)            final or delta location
#
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='loss',
    patience=10,
    restore_best_weights=True,
        mode='min',         # because we want to stop when loss stops decreasing

)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6
)
def distance_loss(y_true, y_pred):
    """
    Custom loss function to compute the Euclidean distance
    between predicted and target positions.

    Args:
        y_true: Tensor of true positions, shape (batch_size, 3).
        y_pred: Tensor of predicted positions, shape (batch_size, 3).

    Returns:
        Mean Euclidean distance over the batch.
    """
    # Compute the squared differences
    squared_differences = tf.square(y_pred - y_true)

    # Sum the squared differences along the last axis (x, y, z)
    sum_squared_differences = tf.reduce_sum(squared_differences, axis=-1)

    # Compute the square root to get Euclidean distance
    distances = tf.sqrt(sum_squared_differences)

    # Return the mean distance over the batch
    return tf.reduce_mean(distances)
model.compile(optimizer='adam', loss=distance_loss, metrics=['mae'])
train_images = preprocess_input(train_images)
val_images=preprocess_input(val_images)
# train_pointed_images=preprocess_input(train_pointed_images)
# val_pointed_images=preprocess_input(val_pointed_images)
# model.fit(
# [train_images, train_relative_start],  # Training inputs
# train_relative_end,                            # Training targets
# batch_size=8,
# epochs=25,
# validation_data=(
#     [val_images, val_relative_start],  # Validation inputs
#     val_relative_end                                # Validation targets
# ),
# callbacks=[reduce_lr,early_stopping]
# )

# Commented out IPython magic to ensure Python compatibility.
# %pip install keras-tuner

tuner = kt.Hyperband(
    build_tunable_model,  # Model-building function
    objective="val_loss",  # Metric to optimize
    max_epochs=30,  # Maximum number of epochs
    factor=2,  # Reduction factor for Hyperband
    directory="/content/drive/MyDrive/hyper_parameter_just_trajectory",  # Directory to save results
    project_name="drone_location_model_tuning"
)

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="logs")

tuner.search(
    [train_images, train_relative_start],  # Training inputs
    train_relative_end,                    # Training targets
    validation_data=([val_images, val_relative_start], val_relative_end),
    epochs=50,
    batch_size=8,
    callbacks=[early_stopping]
)

trials = tuner.oracle.trials

# Iterate over the trials to extract information
for trial_id, trial in trials.items():
    print(f"Trial ID: {trial_id}")
    print(f"Score: {trial.score}")  # Best metric for the trial
    print("Hyperparameters:")
    for hp_name, hp_value in trial.hyperparameters.values.items():
        print(f"  {hp_name}: {hp_value}")
    print("-" * 50)

best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"Best Learning Rate: {best_hps.get('learning_rate')}")
print(f"Best Units Layer 1: {best_hps.get('units_layer_1')}")
print(f"Best Units Layer 2: {best_hps.get('units_layer_2')}")
print(f"Best Dropout Rate: {best_hps.get('dropout_rate')}")
print(f"Best Activation: {best_hps.get('activation')}")

best_model = tuner.hypermodel.build(best_hps)

# Train the best model further if needed
history = best_model.fit(
    [train_images, train_relative_start],
    train_relative_end,
    validation_data=([val_images, val_relative_start], val_relative_end),
    epochs=30,
    batch_size=8,
    callbacks=[reduce_lr, early_stopping]
)

best_model.save("/content/drive/MyDrive/hyper_parameter_just_trajectory/best_model.keras")

from tensorflow.keras.models import load_model
import tensorflow as tf
from tensorflow.keras import layers, models, Input
from tensorflow.keras.applications import MobileNetV2
def distance_loss(y_true, y_pred):
    """
    Custom loss function to compute the Euclidean distance
    between predicted and target positions.

    Args:
        y_true: Tensor of true positions, shape (batch_size, 3).
        y_pred: Tensor of predicted positions, shape (batch_size, 3).

    Returns:
        Mean Euclidean distance over the batch.
    """
    # Compute the squared differences
    squared_differences = tf.square(y_pred - y_true)

    # Sum the squared differences along the last axis (x, y, z)
    sum_squared_differences = tf.reduce_sum(squared_differences, axis=-1)

    # Compute the square root to get Euclidean distance
    distances = tf.sqrt(sum_squared_differences)

    # Return the mean distance over the batch
    return tf.reduce_mean(distances)

# Reload the model with the custom loss function
model = load_model('/content/drive/MyDrive/hyper_parameter_just_trajectory/best_model.keras', custom_objects={'distance_loss': distance_loss})
model.summary()

import numpy as np

# Combine all data for consistent shuffling
dataset = list(zip(train_images, train_relative_start, train_relative_end))

# Shuffle the dataset
np.random.seed(42)  # Set seed for reproducibility
np.random.shuffle(dataset)

# Unpack shuffled data
train_images, train_relative_start, train_relative_end = zip(*dataset)

# Convert back to NumPy arrays
train_images = np.array(train_images)
train_relative_start = np.array(train_relative_start)
train_relative_end = np.array(train_relative_end)

# Define split ratio (e.g., 80% train, 20% validation)
split_ratio = 0.9
split_index = int(len(train_images) * split_ratio)

# Split into training and validation sets
train_images_final = train_images[:split_index]
val_images = train_images[split_index:]

train_relative_start_final = train_relative_start[:split_index]
val_relative_start = train_relative_start[split_index:]

train_relative_end_final = train_relative_end[:split_index]
val_relative_end = train_relative_end[split_index:]

# Print shapes to verify
print("Train Images:", train_images_final.shape)
print("Validation Images:", val_images.shape)
print("Train Start Coords:", train_relative_start_final.shape)
print("Validation Start Coords:", val_relative_start.shape)
print("Train End Coords:", train_relative_end_final.shape)
print("Validation End Coords:", val_relative_end.shape)

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='loss',
    patience=10,
    restore_best_weights=True,
        mode='min',         # because we want to stop when loss stops decreasing

)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6
)
history = model.fit(
    [train_images_final, train_relative_start_final],
    train_relative_end_final,
    validation_data=([val_images, val_relative_start], val_relative_end),
    initial_epoch=0,
    epochs=30,
    batch_size=8,
    callbacks=[reduce_lr, early_stopping]
)

model.save("/content/drive/MyDrive/hyper_parameter_just_trajectory/best_model_data_addition_shuffle.keras")

import keras_tuner as kt

def build_tunable_model(hp):
    """
    Builds the model with tunable hyperparameters.
    """
    # Tunable learning rate
    learning_rate = hp.Float("learning_rate", min_value=1e-4, max_value=1e-2, sampling="log")
    activation_choice = hp.Choice("activation", values=["relu", "tanh", "leaky_relu","elu"])

    # Tunable number of units in Dense layers
    units_layer_1 = hp.Int("units_layer_1", min_value=64, max_value=256, step=32)
    units_layer_2 = hp.Int("units_layer_2", min_value=16, max_value=128, step=16)

    # Tunable dropout rate
    dropout_rate = hp.Float("dropout_rate", min_value=0.1, max_value=0.5, step=0.1)

    # Tunable trainable status for the backbone

    # Define the model
    img_shape = (240, 320, 3)
    loc_shape = (3,)
    image_input = Input(shape=img_shape, name="trajectory_image")
    init_loc_input = Input(shape=loc_shape, name="initial_location")

    # Backbone for image features
    backbone = create_mobilenetv2_backbone(img_shape, trainable=False)
    image_features = backbone(image_input)

    # Combine image features with initial location
    combined = layers.Concatenate()([image_features, init_loc_input])

    # Add regression head with tunable parameters
    x = layers.Dense(units_layer_1)(combined)
    if activation_choice == "leaky_relu":
        alpha_value = hp.Float("leaky_alpha", 0.1, 0.3, step=0.1)
        x = layers.LeakyReLU(alpha=alpha_value)(x)
    elif activation_choice == "elu":
        x = layers.ELU()(x)
    else:
        x = layers.Activation(activation_choice)(x)

    x = layers.Dropout(dropout_rate)(x)

    x = layers.Dense(units_layer_2)(x)
    if activation_choice == "leaky_relu":
        alpha_value = hp.Float("leaky_alpha", 0.1, 0.3, step=0.1)
        x = layers.LeakyReLU(alpha=alpha_value)(x)
    elif activation_choice == "elu":
        x = layers.ELU()(x)
    else:
        x = layers.Activation(activation_choice)(x)
    x = layers.Dropout(dropout_rate)(x)

    outputs = layers.Dense(3, activation='linear', name="end_location")(x)

    # Compile the model
    model = models.Model(inputs=[image_input, init_loc_input], outputs=outputs)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss=distance_loss,
        metrics=['mae']
    )
    return model



from sklearn.model_selection import KFold
k=10

kf = KFold(n_splits=k, shuffle=True, random_state=42)

# Track performance for each fold
fold_metrics = []
for fold, (train_index, val_index) in enumerate(kf.split(train_images)):
    print(f"Fold {fold+1}/{k}")

    # Split into train and validation sets
    X_train_images, X_val_images = train_images[train_index], train_images[val_index]
    X_train_initloc, X_val_initloc = train_relative_start[train_index], train_relative_start[val_index]
    Y_train, Y_val = train_relative_end[train_index], train_relative_end[val_index]

    # Create and compile a new model for each fold
    model = create_model_with_init_location((240, 320, 3), (3,), trainable=False)
    model.compile(optimizer='adam', loss=distance_loss, metrics=['mae'])

    # Train the model
    history = model.fit(
        [X_train_images, X_train_initloc],  # Training inputs
        Y_train,                            # Training targets
        batch_size=8,
        epochs=25,
        validation_data=([X_val_images, X_val_initloc], Y_val),
        callbacks=[reduce_lr, early_stopping],
        verbose=1
    )

    # Evaluate on the validation set
    val_loss, val_mae = model.evaluate([X_val_images, X_val_initloc], Y_val, verbose=0)
    print(f"Validation Loss: {val_loss:.4f}, Validation MAE: {val_mae:.4f}")

    # Store metrics for this fold
    fold_metrics.append((val_loss, val_mae))

# Compute average performance across folds
avg_loss = np.mean([m[0] for m in fold_metrics])
avg_mae = np.mean([m[1] for m in fold_metrics])
print(f"Average Loss: {avg_loss:.4f}, Average MAE: {avg_mae:.4f}")

val_box_centers=np.load(f'{save_path}val_box_centers.npy')

print(np.shape(val_drone_with_traj_images))
print(np.shape(val_images))
print(np.shape(val_observer))
print(np.shape(val_target))
print(np.shape(val_box_centers))

# Save th